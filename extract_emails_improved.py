import re
import urllib.parse
from bs4 import BeautifulSoup
import logging

logger = logging.getLogger(__name__)

def extract_emails_from_html(html_content, url=None):
    """HTMLì—ì„œ ì´ë©”ì¼ ì£¼ì†Œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¶”ì¶œ - ë³µì‚¬ ì œí•œ ë° ë‚œë…í™” ìš°íšŒ í¬í•¨, í™•ì¥ëœ ì´ë©”ì¼ í˜•ì‹ íŒ¨í„´ ì§€ì›"""
    emails = set()
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # 1. í™•ì¥ëœ ì´ë©”ì¼ íŒ¨í„´ ì •ì˜ (ë” ë„“ì€ ë²”ìœ„ì˜ ì´ë©”ì¼ í˜•ì‹ ì¸ì‹)
        email_patterns = [
            # ê¸°ë³¸ ì´ë©”ì¼ íŒ¨í„´
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            # í•œê¸€ ë„ë©”ì¸ ì§€ì› (xn-- í˜•íƒœ)
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]*xn--[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            # ë” ê¸´ TLD ì§€ì› (ìµœëŒ€ 6ìë¦¬)
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,6}\b',
            # ìˆ«ìê°€ í¬í•¨ëœ ë„ë©”ì¸
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]*[0-9]+[A-Za-z0-9.-]*\.[A-Z|a-z]{2,}\b',
            # íŠ¹ìˆ˜ ë¬¸ìê°€ ë” ë§ì´ í¬í•¨ëœ íŒ¨í„´
            r'\b[A-Za-z0-9._%+=-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            # IP ì£¼ì†Œ í˜•íƒœì˜ ë„ë©”ì¸
            r'\b[A-Za-z0-9._%+-]+@\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b',
            # í¬íŠ¸ ë²ˆí˜¸ê°€ í¬í•¨ëœ ê²½ìš°
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}:\d+\b'
        ]
        
        # ëª¨ë“  íŒ¨í„´ìœ¼ë¡œ HTML ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì´ë©”ì¼ ì°¾ê¸°
        text_content = soup.get_text()
        raw_html = str(html_content)
        
        # ê° íŒ¨í„´ë³„ë¡œ ê²€ìƒ‰ ìˆ˜í–‰
        for pattern in email_patterns:
            # í…ìŠ¤íŠ¸ ì½˜í…ì¸ ì—ì„œ ê²€ìƒ‰
            found_emails = re.findall(pattern, text_content, re.IGNORECASE)
            emails.update(found_emails)
            
            # ë³µì‚¬ ì œí•œ ìš°íšŒ: HTML ì†ŒìŠ¤ ì§ì ‘ ê²€ìƒ‰
            raw_emails = re.findall(pattern, raw_html, re.IGNORECASE)
            emails.update(raw_emails)

        
        for pattern in loose_email_patterns:
            loose_emails = re.findall(pattern, text_content + ' ' + raw_html, re.IGNORECASE)
            # ì •ê·œí™”: ê³µë°± ì œê±°, ì „ê° ë¬¸ì ë³€í™˜
            for email in loose_emails:
                normalized = email.strip().replace('ï¼ ', '@').replace('ï¼', '.').replace(' ', '')
                # ê´„í˜¸, ë”°ì˜´í‘œ ì œê±°
                normalized = re.sub(r'^[\(\[\{"\']+|[\)\]\}"\']+$', '', normalized)
                if '@' in normalized and '.' in normalized:
                    emails.add(normalized)

        # 2. í™•ì¥ëœ ë‚œë…í™” íŒ¨í„´ ì²˜ë¦¬
        obfuscated_patterns = [
            # [at], (at), _at_, -at-, {at}, <at> ë“± ë‹¤ì–‘í•œ í˜•íƒœ
            (r'([A-Za-z0-9._%+-]+)[\s]*[\[\(\{\_\-\<]at[\]\)\}\_\-\>][\s]*([A-Za-z0-9.-]+\.[A-Z|a-z]{2,6})', r'\1@\2'),
            # [dot], (dot), _dot_, -dot-, {dot}, <dot> ë“±
            (r'([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]*?)[\s]*[\[\(\{\_\-\<]dot[\]\)\}\_\-\>][\s]*([A-Za-z0-9.-]*\.[A-Z|a-z]{2,6})', r'\1.\2'),
            # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ëœ ì´ë©”ì¼ (ì—¬ëŸ¬ ê³µë°± í¬í•¨)
            (r'([A-Za-z0-9._%+-]+)\s+@\s+([A-Za-z0-9.-]+\.[A-Z|a-z]{2,6})', r'\1@\2'),
            # HTML ì—”í‹°í‹°ë“¤
            (r'([A-Za-z0-9._%+-]+)&#64;([A-Za-z0-9.-]+\.[A-Z|a-z]{2,6})', r'\1@\2'),
            (r'([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]*?)&#46;([A-Za-z0-9.-]*\.[A-Z|a-z]{2,6})', r'\1.\2'),
            # URL ì¸ì½”ë”©
            (r'([A-Za-z0-9._%+-]+)%40([A-Za-z0-9.-]+\.[A-Z|a-z]{2,6})', r'\1@\2'),
            (r'([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]*?)%2E([A-Za-z0-9.-]*\.[A-Z|a-z]{2,6})', r'\1.\2'),
            # ë‹¨ì–´ë¡œ ëœ ë‚œë…í™”
            (r'([A-Za-z0-9._%+-]+)[\s]*\b(at)\b[\s]*([A-Za-z0-9.-]+\.[A-Z|a-z]{2,6})', r'\1@\3'),
            (r'([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]*?)[\s]*\b(dot)\b[\s]*([A-Za-z0-9.-]*\.[A-Z|a-z]{2,6})', r'\1.\3'),
            # í•œê¸€ ë‚œë…í™”
            (r'([A-Za-z0-9._%+-]+)[\s]*ê³¨ë±…ì´[\s]*([A-Za-z0-9.-]+\.[A-Z|a-z]{2,6})', r'\1@\2'),
            (r'([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]*?)[\s]*ì [\s]*([A-Za-z0-9.-]*\.[A-Z|a-z]{2,6})', r'\1.\3'),
            # ë³„í‘œë‚˜ ê¸°íƒ€ íŠ¹ìˆ˜ë¬¸ìë¡œ ëŒ€ì²´
            (r'([A-Za-z0-9._%+-]+)[\s]*[\*\#\$\&][\s]*([A-Za-z0-9.-]+\.[A-Z|a-z]{2,6})', r'\1@\2'),
        ]
        
        # ë‚œë…í™” í•´ì œ ì ìš©
        deobfuscated_text = raw_html + " " + text_content
        for pattern, replacement in obfuscated_patterns:
            deobfuscated_text = re.sub(pattern, replacement, deobfuscated_text, flags=re.IGNORECASE)
        
        # ë‚œë…í™” í•´ì œëœ í…ìŠ¤íŠ¸ì—ì„œ ëª¨ë“  íŒ¨í„´ìœ¼ë¡œ ì´ë©”ì¼ ì¬ê²€ìƒ‰
        for pattern in email_patterns:
            deobfuscated_emails = re.findall(pattern, deobfuscated_text, re.IGNORECASE)
            emails.update(deobfuscated_emails)

        # 3. mailto: ë§í¬ì—ì„œ ì¶”ì¶œ (í™•ì¥ëœ íŒ¨í„´ ì ìš©)
        mailto_links = soup.find_all('a', href=re.compile(r'^mailto:', re.I))
        for link in mailto_links:
            href = link.get('href', '')
            if href.startswith('mailto:'):
                email = href.replace('mailto:', '').split('?')[0]  # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°
                email = email.strip()
                # ëª¨ë“  íŒ¨í„´ìœ¼ë¡œ ê²€ì¦
                for pattern in email_patterns:
                    if re.match(pattern, email, re.IGNORECASE):
                        emails.add(email)
                        break

        # 4. CSSë¡œ ìˆ¨ê²¨ì§„ ì´ë©”ì¼ ì°¾ê¸° (display:none, visibility:hidden ë“±)
        hidden_elements = soup.find_all(attrs={'style': re.compile(r'display\s*:\s*none|visibility\s*:\s*hidden', re.I)})
        for element in hidden_elements:
            hidden_text = element.get_text()
            # ëª¨ë“  íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰
            for pattern in email_patterns:
                hidden_emails = re.findall(pattern, hidden_text, re.IGNORECASE)
                emails.update(hidden_emails)

        # 5. í™•ì¥ëœ íŠ¹ì • íƒœê·¸ì™€ í´ë˜ìŠ¤ì—ì„œ ì´ë©”ì¼ ì°¾ê¸°
        email_selectors = [
            'span[class*="email"]', 'div[class*="email"]', 'p[class*="email"]',
            'span[class*="mail"]', 'div[class*="mail"]', 'p[class*="mail"]',
            'span[class*="contact"]', 'div[class*="contact"]', 'p[class*="contact"]',
            '.contact-info', '.contact-details', '.contact-data',
            '.email', '.mail', '.e-mail',
            '.contact-us', '.contact-form', '.contact-section',
            '.footer-contact', '.footer-info', '.footer-details',
            '[data-email]', '[data-mail]', '[data-contact]',
            '[id*="email"]', '[id*="mail"]', '[id*="contact"]',
            '[class*="info"]', '[class*="detail"]',
            # í•œêµ­ì–´ í´ë˜ìŠ¤ëª…
            '[class*="ì—°ë½"]', '[class*="ë¬¸ì˜"]', '[class*="ì´ë©”ì¼"]',
            # ì¶”ê°€ ì¼ë°˜ì ì¸ ì„ íƒì
            '.about', '.team', '.staff', '.member',
            'address', '.address', '.location',
            '.sidebar', '.widget', '.box'
        ]
        
        for selector in email_selectors:
            try:
                elements = soup.select(selector)
                for element in elements:
                    # í…ìŠ¤íŠ¸ì—ì„œ ëª¨ë“  íŒ¨í„´ìœ¼ë¡œ ì´ë©”ì¼ ì°¾ê¸°
                    element_text = element.get_text()
                    for pattern in email_patterns:
                        found_emails = re.findall(pattern, element_text, re.IGNORECASE)
                        emails.update(found_emails)
                    
                    # HTML ì†ì„±ì—ì„œ ì°¾ê¸° (í™•ì¥ëœ ì†ì„± ëª©ë¡)
                    for attr in ['data-email', 'data-mail', 'data-contact', 'title', 'alt', 'placeholder', 'value', 'content']:
                        if element.get(attr):
                            attr_value = element.get(attr).strip()
                            for pattern in email_patterns:
                                if re.match(pattern, attr_value, re.IGNORECASE):
                                    emails.add(attr_value)
                                    break
            except Exception as e:
                # ì„ íƒì ì˜¤ë¥˜ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                continue

        # 6. í‘¸í„° ë° ì£¼ìš” ì˜ì—­ì—ì„œ ì§‘ì¤‘ ê²€ìƒ‰
        important_sections = soup.find_all(
            ['footer', 'div', 'section', 'aside', 'header'], 
            class_=re.compile(r'footer|contact|info|about|team|staff|member|ì—°ë½|ë¬¸ì˜', re.I)
        )
        for section in important_sections:
            section_text = section.get_text()
            for pattern in email_patterns:
                found_emails = re.findall(pattern, section_text, re.IGNORECASE)
                emails.update(found_emails)

        # 7. ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ ë‚´ ì´ë©”ì¼ (JavaScriptì— ìˆ¨ê²¨ì§„ ê²½ìš°)
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string:
                script_text = script.string
                
                # í™•ì¥ëœ JavaScript ë‚œë…í™” í•´ì œ
                replacements = {
                    '[at]': '@', '(at)': '@', '{at}': '@', '<at>': '@',
                    '[dot]': '.', '(dot)': '.', '{dot}': '.', '<dot>': '.',
                    '_at_': '@', '-at-': '@', '*at*': '@',
                    '_dot_': '.', '-dot-': '.', '*dot*': '.',
                    'ê³¨ë±…ì´': '@', 'ì ': '.'
                }
                
                for old, new in replacements.items():
                    script_text = script_text.replace(old, new)
                
                # í™•ì¥ëœ JavaScript ë¬¸ìì—´ ì—°ê²° íŒ¨í„´ë“¤
                js_patterns = [
                    r'"([^"]+)"\s*\+\s*"@"\s*\+\s*"([^"]+)"',  # "user" + "@" + "domain.com"
                    r"'([^']+)'\s*\+\s*'@'\s*\+\s*'([^']+)'",  # 'user' + '@' + 'domain.com'
                    r'"([^"]+)"\s*\+\s*"\@"\s*\+\s*"([^"]+)"',  # ì´ìŠ¤ì¼€ì´í”„ëœ @
                    r'([a-zA-Z0-9._%-]+)\s*\+\s*"@"\s*\+\s*([a-zA-Z0-9.-]+\.[a-zA-Z]{2,6})',  # ë³€ìˆ˜ + "@" + ë„ë©”ì¸
                ]
                
                for js_pattern in js_patterns:
                    js_matches = re.findall(js_pattern, script_text, re.IGNORECASE)
                    for match in js_matches:
                        potential_email = match[0] + '@' + match[1]
                        # ëª¨ë“  íŒ¨í„´ìœ¼ë¡œ ê²€ì¦
                        for pattern in email_patterns:
                            if re.match(pattern, potential_email, re.IGNORECASE):
                                emails.add(potential_email)
                                break
                
                # ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì§ì ‘ ì´ë©”ì¼ ì°¾ê¸°
                for pattern in email_patterns:
                    found_emails = re.findall(pattern, script_text, re.IGNORECASE)
                    emails.update(found_emails)

        # 8. ì´ë¯¸ì§€ alt í…ìŠ¤íŠ¸ ë° ê¸°íƒ€ ì†ì„±ì—ì„œ ì°¾ê¸°
        images = soup.find_all('img')
        for img in images:
            for attr in ['alt', 'title', 'data-email', 'data-original-title']:
                if img.get(attr):
                    attr_text = img.get(attr)
                    for pattern in email_patterns:
                        found_emails = re.findall(pattern, attr_text, re.IGNORECASE)
                        emails.update(found_emails)
        
        # 9. ë©”íƒ€ íƒœê·¸ì—ì„œ ì°¾ê¸°
        meta_tags = soup.find_all('meta')
        for meta in meta_tags:
            for attr in ['content', 'name', 'property', 'value']:
                if meta.get(attr):
                    meta_value = meta.get(attr)
                    for pattern in email_patterns:
                        found_emails = re.findall(pattern, meta_value, re.IGNORECASE)
                        emails.update(found_emails)
        
        # 10. ì…ë ¥ í•„ë“œì—ì„œ ì°¾ê¸° (placeholder, value ë“±)
        input_fields = soup.find_all(['input', 'textarea'])
        for field in input_fields:
            for attr in ['placeholder', 'value', 'data-placeholder', 'title']:
                if field.get(attr):
                    field_value = field.get(attr)
                    for pattern in email_patterns:
                        found_emails = re.findall(pattern, field_value, re.IGNORECASE)
                        emails.update(found_emails)

        # í•„í„°ë§: í™•ì¥ëœ ì œì™¸ íŒ¨í„´
        filtered_emails = []
        exclude_patterns = [
            r'.*@example\.(com|org|net)',
            r'.*@test\.(com|org|net)',
            r'.*@placeholder\.(com|org|net)',
            r'.*@sample\.(com|org|net)',
            r'.*@demo\.(com|org|net)',
            r'noreply@.*', r'no-reply@.*', r'donotreply@.*',
            r'.*@localhost.*', r'.*@127\.0\.0\.1.*',
            r'.*@\[.*\].*',  # IP ì£¼ì†Œ í˜•íƒœ ì œì™¸
            r'admin@.*', r'webmaster@.*', r'postmaster@.*',
            r'.*@(spam|fake|invalid|null|void)\.',
            r'.*@.*\.(jpg|png|gif|pdf|doc|docx|xls|xlsx)$',  # íŒŒì¼ í™•ì¥ì
        ]
        
        for email in emails:
            email = email.lower().strip()
            if email and not any(re.match(pattern, email, re.IGNORECASE) for pattern in exclude_patterns):
                # ë„ë©”ì¸ì´ í˜„ì¬ ì›¹ì‚¬ì´íŠ¸ì™€ ê´€ë ¨ìˆëŠ”ì§€ í™•ì¸
                if url:
                    domain = email.split('@')[1]
                    url_domain = urllib.parse.urlparse(url).netloc.lower()
                    # ê°™ì€ ë„ë©”ì¸ì´ê±°ë‚˜ ì„œë¸Œë„ë©”ì¸ì¸ ê²½ìš° ìš°ì„ ìˆœìœ„
                    if domain in url_domain or url_domain.endswith(domain):
                        filtered_emails.insert(0, email)  # ì•ì— ì¶”ê°€
                    else:
                        filtered_emails.append(email)
                else:
                    filtered_emails.append(email)
        
        logger.info(f"ğŸ“§ HTMLì—ì„œ {len(filtered_emails)}ê°œ ì´ë©”ì¼ ì¶”ì¶œ: {filtered_emails}")
        return filtered_emails
        
    except Exception as e:
        logger.error(f"ì´ë©”ì¼ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return []
