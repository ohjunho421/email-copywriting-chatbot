"""
í¬íŠ¸ì› ë¸”ë¡œê·¸ ì½˜í…ì¸  ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œ (PostgreSQL)
SQLite ëŒ€ì‹  PostgreSQLì„ ì‚¬ìš©í•˜ì—¬ Railwayì—ì„œ ì˜êµ¬ ì €ì¥
"""

from datetime import datetime
import logging
import json
from collections import Counter
from flask import has_app_context
import requests

logger = logging.getLogger(__name__)

def verify_url_exists(url, timeout=3):
    """
    URLì´ ì‹¤ì œë¡œ ì ‘ê·¼ ê°€ëŠ¥í•œì§€ í™•ì¸ (HEAD ìš”ì²­)
    
    Args:
        url: í™•ì¸í•  URL
        timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    
    Returns:
        bool: URLì´ ì ‘ê·¼ ê°€ëŠ¥í•˜ë©´ True, ì•„ë‹ˆë©´ False
    """
    try:
        response = requests.head(url, timeout=timeout, allow_redirects=True)
        if response.status_code == 200:
            return True
        # 404ë‚˜ ë‹¤ë¥¸ ì—ëŸ¬
        logger.warning(f"âŒ URL ì ‘ê·¼ ì‹¤íŒ¨ ({response.status_code}): {url}")
        return False
    except Exception as e:
        logger.warning(f"âŒ URL ì ‘ê·¼ ì˜¤ë¥˜: {url} - {str(e)}")
        return False

# ëª¨ë“ˆ ë ˆë²¨ì—ì„œ import (app context ì²´í¬ í¬í•¨)
def get_db():
    """Flask appì˜ db ê°ì²´ ê°€ì ¸ì˜¤ê¸° (app context í•„ìˆ˜)"""
    if not has_app_context():
        raise RuntimeError("This function requires Flask app context. Call within 'with app.app_context():'")
    from models import db
    return db

def get_blog_post_model():
    """BlogPost ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (app context í•„ìˆ˜)"""
    if not has_app_context():
        raise RuntimeError("This function requires Flask app context. Call within 'with app.app_context():'")
    from models import BlogPost
    return BlogPost

def get_metadata_model():
    """BlogCacheMetadata ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (app context í•„ìˆ˜)"""
    if not has_app_context():
        raise RuntimeError("This function requires Flask app context. Call within 'with app.app_context():'")
    from models import BlogCacheMetadata
    return BlogCacheMetadata

def init_db():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (SQLAlchemyê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬)"""
    # Flask app contextì—ì„œ db.create_all()ì´ í˜¸ì¶œë˜ë¯€ë¡œ
    # ì—¬ê¸°ì„œëŠ” íŠ¹ë³„í•œ ì‘ì—… ë¶ˆí•„ìš”
    logger.info("âœ… ë¸”ë¡œê·¸ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ (PostgreSQL)")
    return True

def generate_blog_ai_summary(title, content, link):
    """
    ë¸”ë¡œê·¸ ë³¸ë¬¸ì„ AIë¡œ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ìš”ì•½ ìƒì„±
    
    Returns:
        dict: {
            'ai_summary': í•µì‹¬ ìš”ì•½ (2-3ë¬¸ì¥),
            'target_audience': íƒ€ê²Ÿ ê³ ê° ìœ í˜•,
            'key_benefits': í•µì‹¬ íš¨ê³¼/ìˆ˜ì¹˜,
            'pain_points_addressed': í•´ê²°í•˜ëŠ” ë¬¸ì œì ë“¤,
            'case_company': ì‚¬ë¡€ ê³ ê°ì‚¬ëª…,
            'case_industry': ì‚¬ë¡€ ê³ ê°ì‚¬ ì—…ì¢…
        }
    """
    import os
    
    # ë³¸ë¬¸ì´ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
    if not content or len(content) < 100:
        return None
    
    try:
        import google.generativeai as genai
        
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            return None
        
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash')
        
        prompt = f"""ë‹¤ìŒ í¬íŠ¸ì›(PortOne) ë¸”ë¡œê·¸ ê¸€ì„ ë¶„ì„í•´ì„œ JSON í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

**ë¸”ë¡œê·¸ ì œëª©:** {title}
**ë¸”ë¡œê·¸ URL:** {link}
**ë¸”ë¡œê·¸ ë³¸ë¬¸:** {content[:3000]}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”:
{{
    "ai_summary": "ì´ ë¸”ë¡œê·¸ì˜ í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½",
    "target_audience": "ì´ ë¸”ë¡œê·¸ê°€ ë„ì›€ë  ì ì¬ê³ ê° ìœ í˜• (ì˜ˆ: 'ê¸€ë¡œë²Œ ì§„ì¶œ ê³„íš ê¸°ì—…', 'PG ìˆ˜ìˆ˜ë£Œ ì ˆê° ê³ ë¯¼ ê¸°ì—…', 'ì •ì‚° ìë™í™” í•„ìš” ê¸°ì—…')",
    "key_benefits": "ë¸”ë¡œê·¸ì— ì–¸ê¸‰ëœ í•µì‹¬ íš¨ê³¼/ìˆ˜ì¹˜ (ì˜ˆ: 'ìˆ˜ìˆ˜ë£Œ 15% ì ˆê°', 'ê°œë°œ ë¦¬ì†ŒìŠ¤ 85% ì ˆê°', 'ì •ì‚° ì‹œê°„ 90% ë‹¨ì¶•')",
    "pain_points_addressed": "ì´ ë¸”ë¡œê·¸ê°€ í•´ê²°í•˜ëŠ” ë¬¸ì œì ë“¤ (ì˜ˆ: 'PG ë³µìˆ˜ ê´€ë¦¬ ì–´ë ¤ì›€', 'í•´ì™¸ê²°ì œ ì—°ë™ ë³µì¡ì„±', 'ì •ì‚° ìˆ˜ì‘ì—… ë¶€ë‹´')",
    "case_company": "ë¸”ë¡œê·¸ì— ì–¸ê¸‰ëœ ì‚¬ë¡€ ê³ ê°ì‚¬ëª… (ì—†ìœ¼ë©´ null)",
    "case_industry": "ì‚¬ë¡€ ê³ ê°ì‚¬ ì—…ì¢… (ì˜ˆ: 'ì´ì»¤ë¨¸ìŠ¤', 'ê²Œì„', 'ìë™ì°¨', 'íŒ¨ì…˜' ë“±. ì—†ìœ¼ë©´ null)"
}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´."""

        response = model.generate_content(prompt)
        result_text = response.text.strip()
        
        # JSON íŒŒì‹±
        import re
        json_match = re.search(r'\{[\s\S]*\}', result_text)
        if json_match:
            result = json.loads(json_match.group())
            logger.debug(f"âœ… AI ìš”ì•½ ìƒì„±: {title[:30]}...")
            return result
        
        return None
        
    except Exception as e:
        logger.warning(f"âš ï¸ AI ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {title[:30]}... - {str(e)}")
        return None

def save_blog_cache(blog_posts, replace_all=True):
    """
    ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    
    Args:
        blog_posts: ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (dict)
        replace_all: Trueë©´ ê¸°ì¡´ í¬ìŠ¤íŠ¸ ì „ì²´ ì‚­ì œ í›„ ì €ì¥, Falseë©´ ì¶”ê°€/ì—…ë°ì´íŠ¸ë§Œ
    
    Returns:
        bool: ì„±ê³µ ì—¬ë¶€
    """
    try:
        db = get_db()
        BlogPost = get_blog_post_model()
        BlogCacheMetadata = get_metadata_model()
        
        # replace_allì´ Trueë©´ ê¸°ì¡´ í¬ìŠ¤íŠ¸ ì‚­ì œ
        if replace_all:
            db.session.query(BlogPost).delete()
            logger.info("ğŸ—‘ï¸ ê¸°ì¡´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì „ì²´ ì‚­ì œ")
        
        # ìƒˆ í¬ìŠ¤íŠ¸ ì‚½ì… ë˜ëŠ” ì—…ë°ì´íŠ¸
        inserted_count = 0
        updated_count = 0
        
        for post in blog_posts:
            link = post.get('link', '')
            if not link:
                continue
            
            # ê¸°ì¡´ í¬ìŠ¤íŠ¸ í™•ì¸ (linkë¡œ ì¤‘ë³µ ì²´í¬)
            existing_post = db.session.query(BlogPost).filter_by(link=link).first()
            
            if existing_post:
                # ì—…ë°ì´íŠ¸
                existing_post.title = post.get('title', '')
                existing_post.summary = post.get('summary', '')
                existing_post.content = post.get('content', '')
                existing_post.category = post.get('category', '')
                existing_post.keywords = post.get('keywords', '')
                existing_post.industry_tags = post.get('industry_tags', '')
                # ğŸ†• AI ìš”ì•½ ì»¬ëŸ¼ ì—…ë°ì´íŠ¸
                existing_post.ai_summary = post.get('ai_summary', '')
                existing_post.target_audience = post.get('target_audience', '')
                existing_post.key_benefits = post.get('key_benefits', '')
                existing_post.pain_points_addressed = post.get('pain_points_addressed', '')
                existing_post.case_company = post.get('case_company', '')
                existing_post.case_industry = post.get('case_industry', '')
                existing_post.updated_at = datetime.utcnow()
                updated_count += 1
            else:
                # ìƒˆ í¬ìŠ¤íŠ¸ ì‚½ì…
                new_post = BlogPost(
                    title=post.get('title', ''),
                    link=link,
                    summary=post.get('summary', ''),
                    content=post.get('content', ''),
                    category=post.get('category', ''),
                    keywords=post.get('keywords', ''),
                    industry_tags=post.get('industry_tags', ''),
                    # ğŸ†• AI ìš”ì•½ ì»¬ëŸ¼
                    ai_summary=post.get('ai_summary', ''),
                    target_audience=post.get('target_audience', ''),
                    key_benefits=post.get('key_benefits', ''),
                    pain_points_addressed=post.get('pain_points_addressed', ''),
                    case_company=post.get('case_company', ''),
                    case_industry=post.get('case_industry', ''),
                    created_at=datetime.utcnow(),
                    updated_at=datetime.utcnow()
                )
                db.session.add(new_post)
                inserted_count += 1
        
        # ì»¤ë°‹
        db.session.commit()
        
        # ì „ì²´ ê°œìˆ˜ í™•ì¸
        total_count = db.session.query(BlogPost).count()
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        metadata = db.session.query(BlogCacheMetadata).first()
        if metadata:
            metadata.last_updated = datetime.utcnow()
            metadata.posts_count = total_count
        else:
            metadata = BlogCacheMetadata(
                last_updated=datetime.utcnow(),
                posts_count=total_count
            )
            db.session.add(metadata)
        
        db.session.commit()
        
        logger.info(f"âœ… ë¸”ë¡œê·¸ DB ì €ì¥ ì™„ë£Œ: ì‹ ê·œ {inserted_count}ê°œ, ì—…ë°ì´íŠ¸ {updated_count}ê°œ, ì´ {total_count}ê°œ (PostgreSQL)")
        return True
        
    except Exception as e:
        logger.error(f"ë¸”ë¡œê·¸ DB ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        try:
            db.session.rollback()
        except:
            pass
        return False

def generate_ai_summaries_for_existing_blogs(limit=50):
    """
    ê¸°ì¡´ ë¸”ë¡œê·¸ì— AI ìš”ì•½ì´ ì—†ëŠ” ê²ƒë“¤ì— ëŒ€í•´ ì¼ê´„ ìƒì„±
    
    Args:
        limit: í•œ ë²ˆì— ì²˜ë¦¬í•  ìµœëŒ€ ê°œìˆ˜
    
    Returns:
        int: ì—…ë°ì´íŠ¸ëœ ë¸”ë¡œê·¸ ê°œìˆ˜
    """
    try:
        db = get_db()
        BlogPost = get_blog_post_model()
        
        # AI ìš”ì•½ì´ ì—†ëŠ” ë¸”ë¡œê·¸ë§Œ ì¡°íšŒ
        posts_without_summary = db.session.query(BlogPost).filter(
            (BlogPost.ai_summary == None) | (BlogPost.ai_summary == '')
        ).limit(limit).all()
        
        if not posts_without_summary:
            logger.info("âœ… ëª¨ë“  ë¸”ë¡œê·¸ì— AI ìš”ì•½ì´ ìˆìŠµë‹ˆë‹¤")
            return 0
        
        logger.info(f"ğŸ¤– AI ìš”ì•½ ìƒì„± ì‹œì‘: {len(posts_without_summary)}ê°œ ë¸”ë¡œê·¸")
        
        updated_count = 0
        import time
        
        for i, post in enumerate(posts_without_summary):
            try:
                ai_result = generate_blog_ai_summary(
                    post.title,
                    post.content,
                    post.link
                )
                
                if ai_result:
                    post.ai_summary = ai_result.get('ai_summary', '')
                    post.target_audience = ai_result.get('target_audience', '')
                    post.key_benefits = ai_result.get('key_benefits', '')
                    post.pain_points_addressed = ai_result.get('pain_points_addressed', '')
                    post.case_company = ai_result.get('case_company') or ''
                    post.case_industry = ai_result.get('case_industry') or ''
                    post.updated_at = datetime.utcnow()
                    updated_count += 1
                    logger.info(f"   âœ… {i+1}/{len(posts_without_summary)}: {post.title[:40]}...")
                
                # API ê³¼ë¶€í•˜ ë°©ì§€
                time.sleep(0.5)
                
            except Exception as e:
                logger.warning(f"   âš ï¸ ì‹¤íŒ¨: {post.title[:30]}... - {str(e)}")
                continue
        
        db.session.commit()
        logger.info(f"âœ… AI ìš”ì•½ ì¼ê´„ ìƒì„± ì™„ë£Œ: {updated_count}ê°œ ì—…ë°ì´íŠ¸")
        return updated_count
        
    except Exception as e:
        logger.error(f"AI ìš”ì•½ ì¼ê´„ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        try:
            db.session.rollback()
        except:
            pass
        return 0

def load_blog_cache():
    """
    PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ë¡œë“œ
    
    Returns:
        list: ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (dict)
    """
    try:
        db = get_db()
        BlogPost = get_blog_post_model()
        
        # ìµœì‹  ê¸€ë¶€í„° ì¡°íšŒ
        posts_query = db.session.query(BlogPost).order_by(BlogPost.created_at.desc()).all()
        
        if not posts_query:
            logger.info("ğŸ“ ë¸”ë¡œê·¸ DBì— ë°ì´í„° ì—†ìŒ (PostgreSQL)")
            return None
        
        # dict í˜•íƒœë¡œ ë³€í™˜
        posts = []
        for post in posts_query:
            posts.append({
                'title': post.title,
                'link': post.link,
                'summary': post.summary,
                'content': post.content,
                'category': post.category,
                'keywords': post.keywords,
                'industry_tags': post.industry_tags,
                'created_at': post.created_at.isoformat() if post.created_at else None
            })
        
        logger.info(f"ğŸ“š ë¸”ë¡œê·¸ DB ë¡œë“œ ì™„ë£Œ: {len(posts)}ê°œ ê¸€ (PostgreSQL)")
        return posts
        
    except Exception as e:
        logger.error(f"ë¸”ë¡œê·¸ DB ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return None

def get_blog_cache_age():
    """
    ë°ì´í„°ë² ì´ìŠ¤ì˜ ì—…ë°ì´íŠ¸ ì‹œê°„ í™•ì¸
    
    Returns:
        float: ìºì‹œ ë‚˜ì´ (ì‹œê°„ ë‹¨ìœ„) ë˜ëŠ” None
    """
    try:
        db = get_db()
        BlogCacheMetadata = get_metadata_model()
        
        metadata = db.session.query(BlogCacheMetadata).first()
        
        if not metadata or not metadata.last_updated:
            return None
        
        age_hours = (datetime.utcnow() - metadata.last_updated).total_seconds() / 3600
        return age_hours
        
    except Exception as e:
        logger.error(f"ìºì‹œ ì‹œê°„ í™•ì¸ ì˜¤ë¥˜: {str(e)}")
        return None

# ğŸ†• ì•Œë ¤ì§„ ê³ ê°ì‚¬ â†’ ì—…ì¢… ë§¤í•‘ (ë¸”ë¡œê·¸ ì‚¬ë¡€ì—ì„œ ì¶”ì¶œ)
KNOWN_CUSTOMER_INDUSTRIES = {
    # ê²Œì„
    'ë„¥ìŠ¨': 'ê²Œì„', 'ì—”ì”¨ì†Œí”„íŠ¸': 'ê²Œì„', 'nc': 'ê²Œì„', 'ë„·ë§ˆë¸”': 'ê²Œì„', 'í¬ë˜í”„í†¤': 'ê²Œì„',
    'ì¹´ì¹´ì˜¤ê²Œì„ì¦ˆ': 'ê²Œì„', 'ìŠ¤ë§ˆì¼ê²Œì´íŠ¸': 'ê²Œì„', 'í„ì–´ë¹„ìŠ¤': 'ê²Œì„', 'ì»´íˆ¬ìŠ¤': 'ê²Œì„',
    'ë°ë¸Œì‹œìŠ¤í„°ì¦ˆ': 'ê²Œì„', 'ì¿ í‚¤ëŸ°': 'ê²Œì„', 'ìŠˆí¼ì…€': 'ê²Œì„', 'ë¼ì´ì—‡': 'ê²Œì„',
    
    # ì´ì»¤ë¨¸ìŠ¤/ì‡¼í•‘
    'ë¬´ì‹ ì‚¬': 'íŒ¨ì…˜', '29cm': 'íŒ¨ì…˜', 'wì»¨ì…‰': 'íŒ¨ì…˜', 'ì§€ê·¸ì¬ê·¸': 'íŒ¨ì…˜', 'ì—ì´ë¸”ë¦¬': 'íŒ¨ì…˜',
    'ë¸Œëœë””': 'íŒ¨ì…˜', 'í•˜ì´ë²„': 'íŒ¨ì…˜', 'ì˜¤ëŠ˜ì˜ì§‘': 'ì´ì»¤ë¨¸ìŠ¤', 'ë§ˆì¼“ì»¬ë¦¬': 'í‘¸ë“œ', 'ì»¬ë¦¬': 'í‘¸ë“œ',
    'ì¿ íŒ¡': 'ì´ì»¤ë¨¸ìŠ¤', '11ë²ˆê°€': 'ì´ì»¤ë¨¸ìŠ¤', 'ssg': 'ì´ì»¤ë¨¸ìŠ¤', 'ë¡¯ë°ì˜¨': 'ì´ì»¤ë¨¸ìŠ¤',
    'í‹°ëª¬': 'ì´ì»¤ë¨¸ìŠ¤', 'ìœ„ë©”í”„': 'ì´ì»¤ë¨¸ìŠ¤', 'ì¸í„°íŒŒí¬': 'ì´ì»¤ë¨¸ìŠ¤',
    
    # ë·°í‹°
    'ì˜¬ë¦¬ë¸Œì˜': 'ë·°í‹°', 'ì•„ëª¨ë ˆí¼ì‹œí”½': 'ë·°í‹°', 'lgìƒí™œê±´ê°•': 'ë·°í‹°', 'ì´ë‹ˆìŠ¤í”„ë¦¬': 'ë·°í‹°',
    'ì—ë›°ë“œ': 'ë·°í‹°', 'í† ë‹ˆëª¨ë¦¬': 'ë·°í‹°', 'ë¯¸ìƒ¤': 'ë·°í‹°', 'ë”í˜ì´ìŠ¤ìƒµ': 'ë·°í‹°',
    'í™”í•´': 'ë·°í‹°', 'ê¸€ë¡œìš°í”½': 'ë·°í‹°',
    
    # ìë™ì°¨/ëª¨ë¹Œë¦¬í‹°
    'í˜„ëŒ€ìë™ì°¨': 'ìë™ì°¨', 'ê¸°ì•„': 'ìë™ì°¨', 'í˜„ëŒ€ì°¨': 'ìë™ì°¨', 'ì œë„¤ì‹œìŠ¤': 'ìë™ì°¨',
    'ì˜ì¹´': 'ìë™ì°¨', 'íƒ€ë‹¤': 'ìë™ì°¨', 'ì¹´ì¹´ì˜¤ëª¨ë¹Œë¦¬í‹°': 'ìë™ì°¨', 'í‹°ë§µëª¨ë¹Œë¦¬í‹°': 'ìë™ì°¨',
    
    # ì—¬í–‰
    'ì•¼ë†€ì': 'ì—¬í–‰', 'ì—¬ê¸°ì–´ë•Œ': 'ì—¬í–‰', 'ë§ˆì´ë¦¬ì–¼íŠ¸ë¦½': 'ì—¬í–‰', 'í´ë£©': 'ì—¬í–‰',
    'ì¸í„°íŒŒí¬íˆ¬ì–´': 'ì—¬í–‰', 'í•˜ë‚˜íˆ¬ì–´': 'ì—¬í–‰', 'ëª¨ë‘íˆ¬ì–´': 'ì—¬í–‰', 'íŠ¸ë¦½ë‹·ì»´': 'ì—¬í–‰',
    'ì•„ê³ ë‹¤': 'ì—¬í–‰', 'ì—ì–´ë¹„ì•¤ë¹„': 'ì—¬í–‰', 'ìµìŠ¤í”¼ë””ì•„': 'ì—¬í–‰',
    
    # êµìœ¡
    'ë©”ê°€ìŠ¤í„°ë””': 'êµìœ¡', 'ëŒ€ì„±ë§ˆì´ë§¥': 'êµìœ¡', 'ì—ë“€ìœŒ': 'êµìœ¡', 'í´ë˜ìŠ¤101': 'êµìœ¡',
    'íƒˆì‰': 'êµìœ¡', 'í¬ëª½': 'êµìœ¡', 'íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤': 'êµìœ¡', 'ì¸í”„ëŸ°': 'êµìœ¡',
    'ë…¸ë§ˆë“œì½”ë”': 'êµìœ¡', 'ì½”ë“œì‡': 'êµìœ¡',
    
    # ê¸ˆìœµ
    'í† ìŠ¤': 'ê¸ˆìœµ', 'ì¹´ì¹´ì˜¤ë±…í¬': 'ê¸ˆìœµ', 'ì¼€ì´ë±…í¬': 'ê¸ˆìœµ', 'ë±…í¬ìƒëŸ¬ë“œ': 'ê¸ˆìœµ',
    'í•€ë‹¤': 'ê¸ˆìœµ', 'ë Œë”§': 'ê¸ˆìœµ', '8í¼ì„¼íŠ¸': 'ê¸ˆìœµ', 'í”¼í”Œí€ë“œ': 'ê¸ˆìœµ',
    
    # ë¯¸ë””ì–´/ì½˜í…ì¸ 
    'ì™“ì± ': 'ë¯¸ë””ì–´', 'ì›¨ì´ë¸Œ': 'ë¯¸ë””ì–´', 'í‹°ë¹™': 'ë¯¸ë””ì–´', 'ì‹œì¦Œ': 'ë¯¸ë””ì–´',
    'ë©œë¡ ': 'ë¯¸ë””ì–´', 'ì§€ë‹ˆë®¤ì§': 'ë¯¸ë””ì–´', 'í”Œë¡œ': 'ë¯¸ë””ì–´', 'ë°€ë¦¬ì˜ì„œì¬': 'ë¯¸ë””ì–´',
    'ë¦¬ë””ë¶ìŠ¤': 'ë¯¸ë””ì–´', 'ë¦¬ë””': 'ë¯¸ë””ì–´',
    
    # SaaS/B2B
    'í† ìŠ¤í˜ì´ë¨¼ì¸ ': 'SaaS', 'ì±„ë„í†¡': 'SaaS', 'ì„¼ë“œë²„ë“œ': 'SaaS', 'ìŠ¤í‹°ë¹„': 'SaaS',
    'ë…¸ì…˜': 'SaaS', 'ìŠ¬ë™': 'SaaS', 'ì”ë””': 'SaaS', 'í”Œë ‰ìŠ¤': 'SaaS', 'ì‹œí”„í‹°': 'SaaS',
    
    # ë¬¼ë¥˜/ë°°ë‹¬
    'ë°°ë‹¬ì˜ë¯¼ì¡±': 'í‘¸ë“œ', 'ìš”ê¸°ìš”': 'í‘¸ë“œ', 'ì¿ íŒ¡ì´ì¸ ': 'í‘¸ë“œ',
    'cjëŒ€í•œí†µìš´': 'ë¬¼ë¥˜', 'í•œì§„': 'ë¬¼ë¥˜', 'ë¡¯ë°íƒë°°': 'ë¬¼ë¥˜', 'ë¡œì  íƒë°°': 'ë¬¼ë¥˜',
    
    # í”Œë«í¼
    'ì¹´ì¹´ì˜¤': 'í”Œë«í¼', 'ë„¤ì´ë²„': 'í”Œë«í¼', 'ë¼ì¸': 'í”Œë«í¼', 'ë‹¹ê·¼ë§ˆì¼“': 'í”Œë«í¼',
    'ë²ˆê°œì¥í„°': 'ë¦¬ì…€', 'ì¤‘ê³ ë‚˜ë¼': 'ë¦¬ì…€', 'í¬ë¦¼': 'ë¦¬ì…€',
    
    # í—¬ìŠ¤ì¼€ì–´
    'êµ¿ë‹¥': 'í—¬ìŠ¤ì¼€ì–´', 'ë˜‘ë‹¥': 'í—¬ìŠ¤ì¼€ì–´', 'ë‹¥í„°ë‚˜ìš°': 'í—¬ìŠ¤ì¼€ì–´', 'íœ´ë ˆì´í¬ì§€í‹°ë¸Œ': 'í—¬ìŠ¤ì¼€ì–´',
    
    # ë¶€ë™ì‚°
    'ì§ë°©': 'ë¶€ë™ì‚°', 'ë‹¤ë°©': 'ë¶€ë™ì‚°', 'í˜¸ê°±ë…¸ë…¸': 'ë¶€ë™ì‚°', 'ì§‘í† ìŠ¤': 'ë¶€ë™ì‚°',
}


def extract_case_companies_from_blog(content, title=''):
    """
    ë¸”ë¡œê·¸ ë‚´ìš©ì—ì„œ ê³ ê°ì‚¬ë¡€ë¡œ ì–¸ê¸‰ëœ íšŒì‚¬ëª…ê³¼ ì—…ì¢…ì„ ì¶”ì¶œ
    
    Args:
        content: ë¸”ë¡œê·¸ ë³¸ë¬¸
        title: ë¸”ë¡œê·¸ ì œëª©
        
    Returns:
        list: [{'company': 'íšŒì‚¬ëª…', 'industry': 'ì—…ì¢…'}, ...]
    """
    found_companies = []
    text = (title + ' ' + content).lower()
    
    for company, industry in KNOWN_CUSTOMER_INDUSTRIES.items():
        if company.lower() in text:
            found_companies.append({
                'company': company,
                'industry': industry
            })
    
    return found_companies


def extract_keywords_from_post(post):
    """
    ë¸”ë¡œê·¸ ê¸€ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê·œì¹™ ê¸°ë°˜ + ê³ ê°ì‚¬ë¡€ ë¶„ì„)
    
    Args:
        post: ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ dict
    
    Returns:
        tuple: (keywords, industry_tags)
    """
    try:
        content = post.get('content', '')
        title = post.get('title', '')
        
        if not content or len(content) < 50:
            return '', ''
        
        # í‚¤ì›Œë“œ ì´ˆê¸°í™”
        keywords = []
        industry_tags = []
        
        # ì œëª©ê³¼ ë‚´ìš©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì°¾ê¸°
        text_lower = (title + ' ' + content[:2000]).lower()
        
        # ğŸ†• ê³ ê°ì‚¬ë¡€ì—ì„œ íšŒì‚¬ëª… ì¶”ì¶œ â†’ ì—…ì¢… íŒŒì•…
        case_companies = extract_case_companies_from_blog(content, title)
        if case_companies:
            keywords.append('ê³ ê°ì‚¬ë¡€')
            for case in case_companies:
                if case['industry'] not in industry_tags:
                    industry_tags.append(case['industry'])
            logger.debug(f"ë¸”ë¡œê·¸ì—ì„œ ê³ ê°ì‚¬ ë°œê²¬: {[c['company'] for c in case_companies]}")
        
        # ì—…ì¢… ê´€ë ¨ í‚¤ì›Œë“œ (í™•ì¥)
        industry_mapping = {
            'ê²Œì„': ['ê²Œì„', 'game', 'ì¸ì•±ê²°ì œ', 'd2c', 'ì›¹ìƒì ', 'ì•±ìŠ¤í† ì–´', 'êµ¬ê¸€í”Œë ˆì´'],
            'ì´ì»¤ë¨¸ìŠ¤': ['ì´ì»¤ë¨¸ìŠ¤', 'eì»¤ë¨¸ìŠ¤', 'ì‡¼í•‘ëª°', 'commerce', 'ì˜¨ë¼ì¸ëª°', 'ë§ˆì¼“í”Œë ˆì´ìŠ¤', 'ì»¤ë¨¸ìŠ¤', 'ë¦¬í…Œì¼'],
            'ì—¬í–‰': ['ì—¬í–‰', 'travel', 'í•­ê³µ', 'í˜¸í…”', 'ìˆ™ë°•', 'ì˜ˆì•½', 'ota'],
            'êµìœ¡': ['êµìœ¡', 'education', 'ì—ë“€í…Œí¬', 'í•™ì›', 'ê°•ì˜', 'ì˜¨ë¼ì¸êµìœ¡'],
            'ê¸ˆìœµ': ['ê¸ˆìœµ', 'fintech', 'í•€í…Œí¬', 'ë³´í—˜', 'ëŒ€ì¶œ', 'íˆ¬ì'],
            'ë¯¸ë””ì–´': ['ë¯¸ë””ì–´', 'media', 'ì½˜í…ì¸ ', 'ott', 'ìŠ¤íŠ¸ë¦¬ë°', 'êµ¬ë…'],
            'SaaS': ['saas', 'êµ¬ë…ì„œë¹„ìŠ¤', 'subscription', 'ì†Œí”„íŠ¸ì›¨ì–´', 'b2b'],
            'ë¬¼ë¥˜': ['ë¬¼ë¥˜', 'logistics', 'ë°°ì†¡', 'ë°°ë‹¬', 'í’€í•„ë¨¼íŠ¸'],
            'í”Œë«í¼': ['í”Œë«í¼', 'platform', 'ì¤‘ê°œ', 'ë§ˆì¼“', 'íŒŒíŠ¸ë„ˆì •ì‚°'],
            'íŒ¨ì…˜': ['íŒ¨ì…˜', 'fashion', 'ì˜ë¥˜', 'ë¸Œëœë“œ', 'ë¦¬ì…€'],
            'í‘¸ë“œ': ['ìŒì‹', 'food', 'ì‹í’ˆ', 'f&b', 'ë ˆìŠ¤í† ë‘', 'ë°°ë‹¬'],
            'ìë™ì°¨': ['ìë™ì°¨', 'ì°¨ëŸ‰', 'automotive', 'ëª¨ë¹Œë¦¬í‹°'],
            'ë·°í‹°': ['ë·°í‹°', 'í™”ì¥í’ˆ', 'ì½”ìŠ¤ë©”í‹±', 'beauty', 'ìŠ¤í‚¨ì¼€ì–´'],
            'í—¬ìŠ¤ì¼€ì–´': ['ì˜ë£Œ', 'ë³‘ì›', 'í—¬ìŠ¤', 'ê±´ê°•', 'ì œì•½'],
            'ë¶€ë™ì‚°': ['ë¶€ë™ì‚°', 'ê±´ë¬¼', 'ì„ëŒ€', 'ë¶„ì–‘'],
            'ë¦¬ì…€': ['ë¦¬ì…€', 'ì¤‘ê³ ', 'ì„¸ì»¨í•¸ë“œ', 'ë¹ˆí‹°ì§€']
        }
        
        for industry, keywords_list in industry_mapping.items():
            if any(kw in text_lower for kw in keywords_list):
                if industry not in industry_tags:
                    industry_tags.append(industry)
        
        # ê¸°ëŠ¥/í˜œíƒ ê´€ë ¨ í‚¤ì›Œë“œ (í™•ì¥)
        benefit_mapping = {
            'ìˆ˜ìˆ˜ë£Œì ˆê°': ['ìˆ˜ìˆ˜ë£Œ', 'ë¹„ìš©ì ˆê°', 'ì ˆê°', 'í• ì¸', 'ì €ë ´', '15%', '30%'],
            'ê²°ì œì—°ë™': ['ê²°ì œ', 'payment', 'pgì—°ë™', 'api', 'sdk'],
            'ì •ì‚°ìë™í™”': ['ì •ì‚°', 'ë§¤ì¶œ', 'ëŒ€ì‚¬', 'ìë™í™”', 'ì¬ë¬´', 'ë§ˆê°'],
            'PGí†µí•©': ['pg', 'ê°„í¸ê²°ì œ', 'ë©€í‹°pg', 'ë³µìˆ˜pg', '25ê°œ'],
            'ê¸€ë¡œë²Œ': ['í•´ì™¸', 'global', 'ê¸€ë¡œë²Œ', 'í•´ì™¸ê²°ì œ', 'í™˜ìœ¨'],
            'ì •ê¸°ê²°ì œ': ['ì •ê¸°ê²°ì œ', 'subscription', 'ë¹Œë§í‚¤', 'êµ¬ë…ê²°ì œ'],
            'ë¦¬ìŠ¤í¬ê´€ë¦¬': ['ì¥ì• ', 'ë°±ì—…', 'ë¼ìš°íŒ…', 'ë¦¬ìŠ¤í¬', 'ì•ˆì •ì„±'],
            'ê°œë°œíš¨ìœ¨': ['ê°œë°œ', 'ë¦¬ì†ŒìŠ¤', '2ì£¼', '85%', 'íš¨ìœ¨']
        }
        
        for benefit, keywords_list in benefit_mapping.items():
            if any(kw in text_lower for kw in keywords_list):
                keywords.append(benefit)
        
        # ê³ ê°ì‚¬ë¡€ ì—¬ë¶€ í™•ì¸ (ì´ë¯¸ ìœ„ì—ì„œ ì²´í¬í–ˆì§€ë§Œ, í‚¤ì›Œë“œ ê¸°ë°˜ë„ ì¶”ê°€)
        if 'ê³ ê°ì‚¬ë¡€' not in keywords:
            if any(kw in text_lower for kw in ['ê³ ê°ì‚¬', 'ë„ì…ì‚¬ë¡€', 'ì„±ê³µì‚¬ë¡€', 'ì¸í„°ë·°', 'ì¼€ì´ìŠ¤']):
                keywords.append('ê³ ê°ì‚¬ë¡€')
        
        # êµ¬ì²´ì  ìˆ˜ì¹˜ í¬í•¨ ì—¬ë¶€
        import re
        if re.search(r'\d+%|\d+ì–µ|\d+ë§Œì›|\d+ë°°', text_lower):
            keywords.append('ì •ëŸ‰ì íš¨ê³¼')
        
        return ','.join(keywords), ','.join(industry_tags)
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
        return '', ''


def analyze_blog_with_ai(post, gemini_model=None):
    """
    Gemini AIë¡œ ë¸”ë¡œê·¸ ë‚´ìš© ì‹¬ì¸µ ë¶„ì„ (ì„ íƒì  ì‚¬ìš©)
    
    Args:
        post: ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ dict
        gemini_model: Gemini ëª¨ë¸ ê°ì²´
    
    Returns:
        dict: ë¶„ì„ ê²°ê³¼ (target_industry, benefits, case_company, summary)
    """
    if not gemini_model:
        return None
    
    try:
        content = post.get('content', '')[:3000]
        title = post.get('title', '')
        
        prompt = f"""ë‹¤ìŒ í¬íŠ¸ì› ë¸”ë¡œê·¸ ê¸€ì„ ë¶„ì„í•´ì„œ JSONìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.

ì œëª©: {title}
ë‚´ìš©: {content}

ë¶„ì„ í•­ëª©:
1. target_industry: ì´ ê¸€ì´ íƒ€ê²Ÿìœ¼ë¡œ í•˜ëŠ” ì—…ì¢… (ê²Œì„, ì´ì»¤ë¨¸ìŠ¤, ì—¬í–‰, êµìœ¡, ê¸ˆìœµ, SaaS, ë¬¼ë¥˜, í”Œë«í¼, ì¼ë°˜ ì¤‘ íƒ1)
2. main_benefit: ì£¼ìš” í˜œíƒ/ê°€ì¹˜ (ìˆ˜ìˆ˜ë£Œì ˆê°, ê°œë°œíš¨ìœ¨, ì •ì‚°ìë™í™”, ê¸€ë¡œë²Œì§„ì¶œ, ì•ˆì •ì„± ì¤‘ íƒ1)
3. case_company: ì–¸ê¸‰ëœ ê³ ê°ì‚¬ ì´ë¦„ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
4. one_line_summary: í•œ ì¤„ ìš”ì•½ (30ì ì´ë‚´)
5. quantitative_results: ì •ëŸ‰ì  ì„±ê³¼ ìˆ˜ì¹˜ (ì˜ˆ: "ìˆ˜ìˆ˜ë£Œ 15% ì ˆê°", ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{"target_industry": "", "main_benefit": "", "case_company": "", "one_line_summary": "", "quantitative_results": ""}}
"""
        
        response = gemini_model.generate_content(prompt)
        result = response.text.strip()
        
        # JSON íŒŒì‹±
        import json
        if result.startswith('```'):
            result = result.split('```')[1]
            if result.startswith('json'):
                result = result[4:]
        
        return json.loads(result)
        
    except Exception as e:
        logger.error(f"AI ë¸”ë¡œê·¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return None


def reanalyze_all_blog_tags():
    """
    ê¸°ì¡´ ë¸”ë¡œê·¸ ë°ì´í„°ì˜ ì—…ì¢…íƒœê·¸ì™€ í‚¤ì›Œë“œë¥¼ ì¬ë¶„ì„í•˜ì—¬ ì—…ë°ì´íŠ¸
    ì—…ì¢…íƒœê·¸ê°€ ë¹„ì–´ìˆëŠ” ë¸”ë¡œê·¸ì— ëŒ€í•´ ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ íƒœê·¸ ì¶”ì¶œ
    """
    try:
        db = get_db()
        BlogPost = get_blog_post_model()
        
        all_posts = db.session.query(BlogPost).all()
        updated_count = 0
        
        for post in all_posts:
            # í˜„ì¬ ë°ì´í„°ë¡œ ì¬ë¶„ì„
            post_data = {
                'title': post.title or '',
                'content': post.content or '',
                'summary': post.summary or ''
            }
            
            # í‚¤ì›Œë“œì™€ ì—…ì¢…íƒœê·¸ ì¬ì¶”ì¶œ
            new_keywords, new_industry_tags = extract_keywords_from_post(post_data)
            
            # ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
            needs_update = False
            
            # ì—…ì¢…íƒœê·¸ê°€ ë¹„ì–´ìˆëŠ”ë° ìƒˆë¡œ ì¶”ì¶œëœ íƒœê·¸ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
            if not post.industry_tags and new_industry_tags:
                post.industry_tags = new_industry_tags
                needs_update = True
            
            # í‚¤ì›Œë“œê°€ ë¹„ì–´ìˆê±°ë‚˜ ë” í’ë¶€í•´ì§€ë©´ ì—…ë°ì´íŠ¸
            if new_keywords and (not post.keywords or len(new_keywords) > len(post.keywords or '')):
                post.keywords = new_keywords
                needs_update = True
            
            if needs_update:
                updated_count += 1
                logger.info(f"ğŸ“ ë¸”ë¡œê·¸ íƒœê·¸ ì—…ë°ì´íŠ¸: {post.title[:30]}... â†’ ì—…ì¢…: {post.industry_tags}, í‚¤ì›Œë“œ: {post.keywords}")
        
        db.session.commit()
        logger.info(f"âœ… ë¸”ë¡œê·¸ íƒœê·¸ ì¬ë¶„ì„ ì™„ë£Œ: {updated_count}/{len(all_posts)}ê°œ ì—…ë°ì´íŠ¸ë¨")
        return updated_count
        
    except Exception as e:
        logger.error(f"ë¸”ë¡œê·¸ íƒœê·¸ ì¬ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return 0


def get_relevant_blog_posts_by_industry(company_info, max_posts=3, service_type=None, pain_points=None):
    """
    íšŒì‚¬ ì •ë³´ì™€ Pain Pointë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë¸”ë¡œê·¸ ê¸€ ì¡°íšŒ (PostgreSQL)
    
    Args:
        company_info: íšŒì‚¬ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        max_posts: ìµœëŒ€ ë°˜í™˜ ê¸€ ìˆ˜
        service_type: ì„œë¹„ìŠ¤ íƒ€ì… ('OPI', 'PS', 'PRISM' ë“±)
        pain_points: Pain Point í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['êµ¬ë…ê²°ì œ', 'PGê´€ë¦¬', 'ì •ì‚°'])
    
    Returns:
        list: ê´€ë ¨ ë¸”ë¡œê·¸ ê¸€ ë¦¬ìŠ¤íŠ¸
    """
    try:
        db = get_db()
        BlogPost = get_blog_post_model()
        
        # íšŒì‚¬ ì •ë³´ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ
        industry = company_info.get('industry', '')
        category = company_info.get('category', '')
        description = company_info.get('description', '')
        
        search_terms = []
        if industry:
            search_terms.append(industry)
        if category:
            search_terms.append(category)
        
        # Pain Point í‚¤ì›Œë“œ ì¶”ê°€ (ìµœìš°ì„ )
        pain_point_terms = []
        if pain_points:
            pain_point_terms.extend(pain_points)
            logger.info(f"ğŸ¯ Pain Point í‚¤ì›Œë“œ: {', '.join(pain_points)}")
        
        # ì„¤ëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        if description:
            desc_lower = description.lower()
            for keyword in ['ê²Œì„', 'game', 'ì´ì»¤ë¨¸ìŠ¤', 'ì‡¼í•‘ëª°', 'ì—¬í–‰', 'travel', 'êµìœ¡', 'education', 'ê¸ˆìœµ', 'fintech']:
                if keyword in desc_lower:
                    search_terms.append(keyword)
        
        from sqlalchemy import or_
        
        # ë‘ ë‹¨ê³„ ê²€ìƒ‰: 1) Pain Point ë§¤ì¹­ ìš°ì„  2) ì—…ì¢… ë§¤ì¹­
        all_posts = []
        seen_ids = set()
        
        # 1ë‹¨ê³„: Pain Point í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ (ìµœìš°ì„ )
        if pain_point_terms:
            pain_query = db.session.query(BlogPost)
            if service_type:
                pain_query = pain_query.filter(BlogPost.category == service_type)
            
            pain_pattern = f"%{'%'.join(pain_point_terms)}%"
            pain_query = pain_query.filter(
                or_(
                    BlogPost.keywords.like(pain_pattern),
                    BlogPost.title.like(pain_pattern),
                    BlogPost.content.like(pain_pattern)
                )
            )
            
            pain_posts = pain_query.order_by(BlogPost.created_at.desc()).limit(max_posts).all()
            for post in pain_posts:
                if post.id not in seen_ids:
                    all_posts.append(post)
                    seen_ids.add(post.id)
                    logger.info(f"  âœ… Pain Point ë§¤ì¹­: {post.title[:50]}...")
        
        # 2ë‹¨ê³„: ì—…ì¢… í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ (Pain Point ë§¤ì¹­ í›„ ë¶€ì¡±í•˜ë©´ ì±„ìš°ê¸°)
        remaining_count = max_posts - len(all_posts)
        if remaining_count > 0 and search_terms:
            industry_query = db.session.query(BlogPost)
            if service_type:
                industry_query = industry_query.filter(BlogPost.category == service_type)
            
            search_pattern = f"%{'%'.join(search_terms)}%"
            industry_query = industry_query.filter(
                or_(
                    BlogPost.industry_tags.like(search_pattern),
                    BlogPost.keywords.like(search_pattern),
                    BlogPost.title.like(search_pattern),
                    BlogPost.content.like(search_pattern)
                )
            )
            
            industry_posts = industry_query.order_by(BlogPost.created_at.desc()).limit(remaining_count).all()
            for post in industry_posts:
                if post.id not in seen_ids:
                    all_posts.append(post)
                    seen_ids.add(post.id)
        
        posts_query = all_posts
        
        service_label = f"[{service_type}] " if service_type else ""
        
        if not posts_query:
            if search_terms:
                logger.info(f"ğŸ” {service_label}'{', '.join(search_terms)}' ê´€ë ¨ ë¸”ë¡œê·¸ ê¸€ ì—†ìŒ")
            else:
                logger.info(f"ğŸ” {service_label}ë¸”ë¡œê·¸ ê¸€ ì—†ìŒ")
            return []
        
        # dict í˜•íƒœë¡œ ë³€í™˜
        posts = []
        for post in posts_query:
            posts.append({
                'title': post.title,
                'link': post.link,
                'summary': post.summary,
                'content': post.content,
                'category': post.category,
                'keywords': post.keywords,
                'industry_tags': post.industry_tags
            })
        
        if search_terms:
            logger.info(f"âœ… {service_label}'{', '.join(search_terms)}' ê´€ë ¨ ë¸”ë¡œê·¸ ê¸€ {len(posts)}ê°œ ì¡°íšŒ (PostgreSQL)")
        else:
            logger.info(f"âœ… {service_label}ë¸”ë¡œê·¸ ê¸€ {len(posts)}ê°œ ì¡°íšŒ (PostgreSQL)")
        
        return posts
        
    except Exception as e:
        logger.error(f"ì—…ì¢…ë³„ ë¸”ë¡œê·¸ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return []

def get_best_blog_for_email_mention(company_info, research_data=None, max_check=50, competitors=None, service_type=None):
    """
    ì´ë©”ì¼ ë³¸ë¬¸ì— ì–¸ê¸‰í•  ê°€ì¥ ì í•©í•œ ë¸”ë¡œê·¸ 1ê°œ ì„ íƒ
    
    ì„ íƒ ê¸°ì¤€ (ìš°ì„ ìˆœìœ„):
    1. ê²½ìŸì‚¬ ì‚¬ë¡€ ë¸”ë¡œê·¸ (ê°€ì¥ ì„¤ë“ë ¥ ìˆìŒ)
    2. ë™ì¼ ì—…ì¢…ì˜ ìœ ì‚¬ ê¸°ì—… ì‚¬ë¡€
    3. ê´€ë ¨ ì‚°ì—…ì˜ í•´ê²° ì‚¬ë¡€
    4. ë°›ì„ ìˆ˜ ìˆëŠ” í˜œíƒ(ìˆ˜ìˆ˜ë£Œ ì ˆê°, ìë™í™” ë“±)ê³¼ ê´€ë ¨ëœ ì •ë³´
    
    Args:
        company_info: íšŒì‚¬ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        research_data: ì¡°ì‚¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (pain_points ë“±)
        max_check: í™•ì¸í•  ìµœëŒ€ ë¸”ë¡œê·¸ ìˆ˜
        competitors: ê²½ìŸì‚¬ ë¦¬ìŠ¤íŠ¸ (ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸)
        service_type: ì„œë¹„ìŠ¤ ìœ í˜• ('OPI', 'PS', 'PRISM' ë“±) - í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë¸”ë¡œê·¸ë§Œ ë§¤ì¹­
    
    Returns:
        dict or None: ì„ íƒëœ ë¸”ë¡œê·¸ ì •ë³´ (title, link, summary, match_reason)
    """
    try:
        db = get_db()
        BlogPost = get_blog_post_model()
        
        # íšŒì‚¬ ì •ë³´ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ
        industry = company_info.get('industry', '') or ''
        category = company_info.get('category', '') or ''
        description = company_info.get('description', '') or ''
        company_name = company_info.get('company_name', '') or company_info.get('íšŒì‚¬ëª…', '') or ''
        
        # ğŸ†• BM ë¶„ì„ ê²°ê³¼ì—ì„œ ì—…ì¢… ì •ë³´ ì¶”ì¶œ (Perplexity ì¡°ì‚¬ ê²°ê³¼ - ê°€ì¥ ì •í™•)
        bm_analysis = company_info.get('business_model', {})
        bm_primary = bm_analysis.get('primary_model_kr', '')  # ì˜ˆ: 'ì´ì»¤ë¨¸ìŠ¤/ì‡¼í•‘ëª°'
        bm_detected = bm_analysis.get('detected_models', [])  # ì˜ˆ: ['ecommerce', 'overseas']
        
        # BM ë¶„ì„ ê²°ê³¼ë¥¼ ì—…ì¢… ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        bm_industries = []
        bm_to_industry_map = {
            'ecommerce': 'ì´ì»¤ë¨¸ìŠ¤',
            'subscription': 'SaaS',
            'mobile_app': 'ê²Œì„',
            'platform': 'í”Œë«í¼',
            'overseas': 'ê¸€ë¡œë²Œ',
            'b2b': 'B2B',
            'content': 'ë¯¸ë””ì–´'
        }
        for bm in bm_detected:
            if bm in bm_to_industry_map:
                bm_industries.append(bm_to_industry_map[bm])
        
        if bm_primary:
            logger.info(f"ğŸ“Š BM ë¶„ì„ ê¸°ë°˜ ì—…ì¢…: {bm_primary} â†’ {bm_industries}")
        
        # research_dataì—ì„œ pain_points, company_info ì¶”ì¶œ
        pain_points = ''
        research_company_info = ''
        if research_data:
            pain_points = research_data.get('pain_points', '') or ''
            research_company_info = research_data.get('company_info', '') or ''
        
        # ê²½ìŸì‚¬ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
        competitor_list = []
        if competitors:
            if isinstance(competitors, str):
                # ì‰¼í‘œ, ìŠ¬ë˜ì‹œ, ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
                import re
                competitor_list = [c.strip().lower() for c in re.split(r'[,/\s]+', competitors) if c.strip() and len(c.strip()) > 1]
            elif isinstance(competitors, list):
                competitor_list = [c.lower() for c in competitors if c and len(c) > 1]
        
        logger.info(f"ğŸ” ë¸”ë¡œê·¸ ë§¤ì¹­ - ê²½ìŸì‚¬ ë¦¬ìŠ¤íŠ¸: {competitor_list}")
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸° (ë‰´ìŠ¤ ê¸°ì‚¬ í¬í•¨)
        news_content = research_data.get('news_summary', '') or research_data.get('news', '') or '' if research_data else ''
        all_text = f"{company_name} {industry} {category} {description} {pain_points} {research_company_info} {news_content}".lower()
        
        # ğŸ†• ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ íšŒì‚¬ì˜ 'ì˜ë„/ê³„íš' íŒŒì•… â†’ ì‹œë‚˜ë¦¬ì˜¤ ë§¤ì¹­
        # âš ï¸ ì˜ë„ ë§¤ì¹­ì€ ìµœìš°ì„ ! ì ìˆ˜ë¥¼ í¬ê²Œ ë†’ì—¬ í™•ì‹¤íˆ ì„ íƒë˜ë„ë¡ í•¨
        intent_scenarios = {
            'ê¸€ë¡œë²Œì§„ì¶œ': {
                'keywords': ['í•´ì™¸ì§„ì¶œ', 'ì¼ë³¸ì§„ì¶œ', 'ê¸€ë¡œë²Œ', 'í•´ì™¸ì‹œì¥', 'ìˆ˜ì¶œ', 'ë¯¸êµ­ì§„ì¶œ', 'ë™ë‚¨ì•„', 'ì¤‘êµ­ì§„ì¶œ', 'í¬ë¡œìŠ¤ë³´ë”', 'í˜„ì§€í™”', 'í•´ì™¸ë§¤ì¶œ', 'ê¸€ë¡œë²Œí™•ì¥', 'uae', 'ë² íŠ¸ë‚¨', 'íƒœêµ­', 'ì¸ë„ë„¤ì‹œì•„', 'ì‹±ê°€í¬ë¥´', 'í•´ì™¸', 'ë‹¤êµ­ê°€'],
                'blog_keywords': ['ê¸€ë¡œë²Œ', 'í•´ì™¸', 'global', 'ì¼ë³¸', 'í¬ë¡œìŠ¤ë³´ë”', 'ìš°ì»¤ë¨¸ìŠ¤', 'woocommerce'],
                'score': 100  # ğŸ”¥ ì˜ë„ ë§¤ì¹­ì€ ìµœìš°ì„  (ë‹¤ë¥¸ ë§¤ì¹­ë³´ë‹¤ í™•ì‹¤íˆ ë†’ê²Œ)
            },
            'êµ¬ë…ì„œë¹„ìŠ¤': {
                'keywords': ['êµ¬ë…', 'ì •ê¸°ê²°ì œ', 'ë©¤ë²„ì‹­', 'saas', 'ott', 'ì›”ì •ì•¡', 'êµ¬ë…ëª¨ë¸', 'ì •ê¸°ë°°ì†¡', 'êµ¬ë…ê²½ì œ'],
                'blog_keywords': ['ë¹Œë§í‚¤', 'êµ¬ë…', 'ì •ê¸°ê²°ì œ', 'subscription'],
                'score': 100
            },
            'ì •ì‚°ê°œì„ ': {
                'keywords': ['ì •ì‚°', 'ë§¤ì¶œê´€ë¦¬', 'ì¬ë¬´', 'íšŒê³„', 'ëŒ€ì‚¬', 'ë§ˆê°', 'erp', 'ìë™í™”', 'íš¨ìœ¨í™”'],
                'blog_keywords': ['ì •ì‚°', 'ë§¤ì¶œ', 'ìë™í™”', 'ëŒ€ì‚¬', 'ë§ˆê°'],
                'score': 100
            },
            'ê²°ì œì—°ë™': {
                'keywords': ['ê²°ì œë„ì…', 'pgì—°ë™', 'ê²°ì œì‹œìŠ¤í…œ', 'ê²°ì œìˆ˜ë‹¨', 'ê°„í¸ê²°ì œ', 'í˜ì´', 'ê²°ì œì†”ë£¨ì…˜'],
                'blog_keywords': ['ê²°ì œ', 'pg', 'ì—°ë™', 'api'],
                'score': 80
            },
            'ë¹„ìš©ì ˆê°': {
                'keywords': ['ìˆ˜ìˆ˜ë£Œ', 'ë¹„ìš©ì ˆê°', 'ì›ê°€', 'íš¨ìœ¨', 'ì¸ì•±ê²°ì œ', 'ìˆ˜ìˆ˜ë£Œì¸í•˜'],
                'blog_keywords': ['ìˆ˜ìˆ˜ë£Œ', 'ì ˆê°', '30%', 'ë¹„ìš©'],
                'score': 80
            }
        }
        
        # ë‰´ìŠ¤ì—ì„œ íŒŒì•…ëœ ì˜ë„ ì°¾ê¸°
        detected_intents = []
        for intent_name, intent_info in intent_scenarios.items():
            for kw in intent_info['keywords']:
                if kw in all_text:
                    detected_intents.append(intent_name)
                    break
        
        if detected_intents:
            logger.info(f"ğŸ“° ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ íŒŒì•…ëœ íšŒì‚¬ ì˜ë„: {detected_intents}")
        
        # ğŸ†• í™•ì¥ëœ ì‚°ì—… í‚¤ì›Œë“œ ë§¤ì¹­ (ë” ì„¸ë¶„í™”)
        industry_keywords = {
            # IT/í…Œí¬
            'ê²Œì„': ['ê²Œì„', 'game', 'ì¸ì•±ê²°ì œ', 'd2c', 'ì›¹ìƒì ', 'ì•±ìŠ¤í† ì–´', 'êµ¬ê¸€í”Œë ˆì´', 'ìŠ¤íŒ€'],
            'SaaS': ['saas', 'b2b', 'ì†Œí”„íŠ¸ì›¨ì–´', 'í´ë¼ìš°ë“œ', 'ì†”ë£¨ì…˜', 'í”Œë«í¼ì„œë¹„ìŠ¤'],
            'ITì„œë¹„ìŠ¤': ['it', 'í…Œí¬', 'tech', 'ì†Œí”„íŠ¸ì›¨ì–´', 'ê°œë°œ', 'ìŠ¤íƒ€íŠ¸ì—…'],
            
            # ì»¤ë¨¸ìŠ¤
            'ì´ì»¤ë¨¸ìŠ¤': ['ì´ì»¤ë¨¸ìŠ¤', 'eì»¤ë¨¸ìŠ¤', 'ì‡¼í•‘ëª°', 'ì»¤ë¨¸ìŠ¤', 'ì˜¨ë¼ì¸ëª°', 'ë§ˆì¼“í”Œë ˆì´ìŠ¤', 'ì˜¨ë¼ì¸ì‡¼í•‘'],
            'ë¦¬ì…€/ì¤‘ê³ ': ['ë¦¬ì…€', 'ì¤‘ê³ ', 'ì„¸ì»¨í•¸ë“œ', 'ë¹ˆí‹°ì§€', 'ë²ˆê°œì¥í„°', 'ë‹¹ê·¼'],
            'íŒ¨ì…˜': ['íŒ¨ì…˜', 'fashion', 'ì˜ë¥˜', 'ë¸Œëœë“œ', 'ì˜·', 'ì‹ ë°œ', 'ì•¡ì„¸ì„œë¦¬'],
            'ë·°í‹°': ['ë·°í‹°', 'í™”ì¥í’ˆ', 'ì½”ìŠ¤ë©”í‹±', 'beauty', 'ìŠ¤í‚¨ì¼€ì–´', 'ë©”ì´í¬ì—…'],
            
            # ì—¬í–‰/ìˆ™ë°•
            'ì—¬í–‰': ['ì—¬í–‰', 'travel', 'í•­ê³µ', 'í˜¸í…”', 'ìˆ™ë°•', 'ì˜ˆì•½', 'ota', 'íˆ¬ì–´'],
            
            # êµìœ¡
            'êµìœ¡': ['êµìœ¡', 'education', 'ì—ë“€í…Œí¬', 'í•™ì›', 'ê°•ì˜', 'ì˜¨ë¼ì¸ê°•ì˜', 'ì´ëŸ¬ë‹'],
            
            # ê¸ˆìœµ
            'ê¸ˆìœµ': ['ê¸ˆìœµ', 'fintech', 'í•€í…Œí¬', 'ë³´í—˜', 'ëŒ€ì¶œ', 'íˆ¬ì', 'ì¦ê¶Œ', 'ì€í–‰'],
            
            # ë¯¸ë””ì–´/ì½˜í…ì¸ 
            'ë¯¸ë””ì–´': ['ë¯¸ë””ì–´', 'media', 'ì½˜í…ì¸ ', 'ott', 'ìŠ¤íŠ¸ë¦¬ë°', 'ì˜ìƒ', 'ë‰´ìŠ¤'],
            'ì—”í„°í…Œì¸ë¨¼íŠ¸': ['ì—”í„°', 'ì—°ì˜ˆ', 'ê³µì—°', 'í‹°ì¼“', 'ì½˜ì„œíŠ¸', 'ì˜í™”'],
            
            # ë¬¼ë¥˜/ë°°ì†¡
            'ë¬¼ë¥˜': ['ë¬¼ë¥˜', 'logistics', 'ë°°ì†¡', 'ë°°ë‹¬', 'í’€í•„ë¨¼íŠ¸', 'íƒë°°'],
            'í‘¸ë“œ': ['ìŒì‹', 'food', 'ì‹í’ˆ', 'f&b', 'ë ˆìŠ¤í† ë‘', 'ë°°ë‹¬', 'ì‹ìì¬'],
            
            # í”Œë«í¼
            'í”Œë«í¼': ['í”Œë«í¼', 'platform', 'ì¤‘ê°œ', 'ë§ˆì¼“', 'íŒŒíŠ¸ë„ˆì •ì‚°'],
            
            # ì œì¡°/ì‚°ì—…
            'ìë™ì°¨': ['ìë™ì°¨', 'ì°¨ëŸ‰', 'automotive', 'ëª¨ë¹Œë¦¬í‹°', 'ì¹´ì‰ì–´', 'ìë™ì°¨ë³´í—˜'],  # 'ì¹´', 'ì˜¤í† 'ëŠ” ë„ˆë¬´ ê´‘ë²”ìœ„í•´ì„œ ì œì™¸
            'ì œì¡°': ['ì œì¡°', 'manufacturing', 'ê³µì¥', 'ìƒì‚°', 'ë¶€í’ˆ'],
            
            # í—¬ìŠ¤ì¼€ì–´
            'í—¬ìŠ¤ì¼€ì–´': ['ì˜ë£Œ', 'ë³‘ì›', 'í—¬ìŠ¤', 'ê±´ê°•', 'ì œì•½', 'ë°”ì´ì˜¤'],
            
            # ë¶€ë™ì‚°
            'ë¶€ë™ì‚°': ['ë¶€ë™ì‚°', 'ê±´ë¬¼', 'ì„ëŒ€', 'ë¶„ì–‘', 'ì¤‘ê°œ'],
            
            # ê¸€ë¡œë²Œ
            'ê¸€ë¡œë²Œ': ['í•´ì™¸', 'ê¸€ë¡œë²Œ', 'global', 'ìˆ˜ì¶œ', 'í•´ì™¸ì§„ì¶œ', 'í¬ë¡œìŠ¤ë³´ë”']
        }
        
        # ğŸ†• ì—…ì¢… ìœ ì‚¬ë„ ê·¸ë£¹ (ê°™ì€ ê·¸ë£¹ ë‚´ ì—…ì¢…ì€ ì—°ê´€ì„± ìˆìŒ, ë‹¤ë¥¸ ê·¸ë£¹ì€ ë„ˆë¬´ ë‹¤ë¦„)
        # ì˜ˆ: ë·°í‹° íšŒì‚¬ â†’ ì´ì»¤ë¨¸ìŠ¤/íŒ¨ì…˜ ë¸”ë¡œê·¸ OK, ìë™ì°¨ ë¸”ë¡œê·¸ NO
        similar_industry_groups = [
            ['ì´ì»¤ë¨¸ìŠ¤', 'ë·°í‹°', 'íŒ¨ì…˜', 'ë¦¬ì…€/ì¤‘ê³ '],  # ì»¤ë¨¸ìŠ¤/ì†Œë¹„ì¬ ê·¸ë£¹
            ['ìë™ì°¨', 'ì œì¡°', 'ë¬¼ë¥˜'],                # ì œì¡°/ì‚°ì—… ê·¸ë£¹  
            ['ê²Œì„', 'ì—”í„°í…Œì¸ë¨¼íŠ¸', 'ë¯¸ë””ì–´'],        # ì½˜í…ì¸ /ì—”í„° ê·¸ë£¹
            ['SaaS', 'ITì„œë¹„ìŠ¤'],                     # IT/í…Œí¬ ê·¸ë£¹
            ['ì—¬í–‰', 'í‘¸ë“œ'],                         # ì„œë¹„ìŠ¤ì—… ê·¸ë£¹
            ['ê¸ˆìœµ'],                                 # ê¸ˆìœµ ê·¸ë£¹ (ë…ë¦½ì )
            ['í—¬ìŠ¤ì¼€ì–´'],                             # ì˜ë£Œ ê·¸ë£¹ (ë…ë¦½ì )
            ['ë¶€ë™ì‚°'],                               # ë¶€ë™ì‚° ê·¸ë£¹ (ë…ë¦½ì )
            ['êµìœ¡'],                                 # êµìœ¡ ê·¸ë£¹
            ['í”Œë«í¼'],                               # í”Œë«í¼ (ë²”ìš©ì )
            ['ê¸€ë¡œë²Œ'],                               # ê¸€ë¡œë²Œ (ë²”ìš©ì  - ëª¨ë“  ì—…ì¢…ê³¼ í˜¸í™˜)
        ]
        
        # í˜œíƒ í‚¤ì›Œë“œ ë§¤ì¹­ (ì„¸ì¼ì¦ˆ ì‹œë‚˜ë¦¬ì˜¤ë³„ ê°•í™”)
        benefit_keywords = {
            # ğŸŒ ê¸€ë¡œë²Œ ì§„ì¶œ ì‹œë‚˜ë¦¬ì˜¤
            'ê¸€ë¡œë²Œ': ['í•´ì™¸', 'ê¸€ë¡œë²Œ', 'global', 'í•´ì™¸ê²°ì œ', 'í™˜ìœ¨', 'í¬ë¡œìŠ¤ë³´ë”', 'ì¼ë³¸', 'ë™ë‚¨ì•„', 'ë¯¸êµ­', 'ì¤‘êµ­', 'paypay', 'alipay', 'ì§„ì¶œ', 'ìˆ˜ì¶œ'],
            # ğŸ”„ êµ¬ë… ì„œë¹„ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤
            'êµ¬ë…': ['êµ¬ë…', 'saas', 'ë©¤ë²„ì‹­', 'ì •ê¸°ê²°ì œ', 'ë¹Œë§í‚¤', 'ë¹Œë§', 'ott', 'ì •ê¸°ë°°ì†¡', 'ì›”ì •ì•¡', 'ì—°ê°„êµ¬ë…'],
            # 1ï¸âƒ£ ë‹¨ì¼/ë³µìˆ˜ PG ì‹œë‚˜ë¦¬ì˜¤
            'ìˆ˜ìˆ˜ë£Œì ˆê°': ['ìˆ˜ìˆ˜ë£Œ', 'ë¹„ìš©', 'ì ˆê°', 'í• ì¸', 'ì €ë ´', '15%', '30%', 'ë‹¨ì¼pg', 'ë©€í‹°pg', 'ë³µìˆ˜pg'],
            'ì •ì‚°': ['ì •ì‚°', 'ë§¤ì¶œ', 'ì¬ë¬´', 'íšŒê³„', 'ëŒ€ì‚¬', 'ëŒ€ì‹œë³´ë“œ', 'í†µí•©ê´€ë¦¬', 'ìë™ëŒ€ì‚¬'],
            # ê³µí†µ
            'ìë™í™”': ['ìë™í™”', 'ìë™', 'íš¨ìœ¨', 'ë¦¬ì†ŒìŠ¤', 'ì‹œê°„ì ˆì•½', '90%', 'ë‹¨ì¶•'],
            'ì•ˆì •ì„±': ['ì•ˆì •', 'ì¥ì• ', 'ë¦¬ìŠ¤í¬', 'ë°±ì—…', 'ë¼ìš°íŒ…', 'ìŠ¤ë§ˆíŠ¸ë¼ìš°íŒ…', 'ìë™ì „í™˜', 'ì´íƒˆë¥ '],
            'ê°œë°œíš¨ìœ¨': ['ê°œë°œ', 'api', 'sdk', 'ì—°ë™', '2ì£¼', '85%', 'êµ¬ì¶•']
        }
        
        # ğŸ†• íšŒì‚¬ì— í•´ë‹¹í•˜ëŠ” ì‚°ì—… ì°¾ê¸° - BM ë¶„ì„ ê²°ê³¼ ìš°ì„  ì‚¬ìš©!
        matched_industries = []
        
        # 1ìˆœìœ„: BM ë¶„ì„ ê²°ê³¼ (Perplexityê°€ íŒŒì•…í•œ ì—…ì¢… - ê°€ì¥ ì •í™•)
        if bm_industries:
            matched_industries = bm_industries.copy()
            logger.info(f"âœ… BM ë¶„ì„ ê¸°ë°˜ ì—…ì¢… ì‚¬ìš©: {matched_industries}")
        else:
            # 2ìˆœìœ„: í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­ (í´ë°±)
            for ind, keywords in industry_keywords.items():
                for kw in keywords:
                    if kw in all_text:
                        matched_industries.append(ind)
                        break
            if matched_industries:
                logger.info(f"âš ï¸ BM ë¶„ì„ ì—†ìŒ - í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°±: {matched_industries}")
        
        # ê´€ì‹¬ í˜œíƒ ì°¾ê¸°
        matched_benefits = []
        for benefit, keywords in benefit_keywords.items():
            for kw in keywords:
                if kw in all_text:
                    matched_benefits.append(benefit)
                    break
        
        logger.info(f"ğŸ¯ ë¸”ë¡œê·¸ ì„ íƒ - íšŒì‚¬: {company_name}, ë§¤ì¹­ëœ ì‚°ì—…: {matched_industries}, í˜œíƒ: {matched_benefits}")
        
        # ğŸ†• íšŒì‚¬ì˜ ì—…ì¢… ìœ ì‚¬ë„ ê·¸ë£¹ ì°¾ê¸°
        company_similar_groups = []
        for group in similar_industry_groups:
            if any(ind in matched_industries for ind in group):
                company_similar_groups.append(group)
        
        # ì—…ì¢…ì´ ì—†ìœ¼ë©´ ë²”ìš©ì  ê·¸ë£¹(í”Œë«í¼, ê¸€ë¡œë²Œ)ê³¼ í˜¸í™˜
        if not company_similar_groups:
            company_similar_groups = [['í”Œë«í¼'], ['ê¸€ë¡œë²Œ'], ['ì´ì»¤ë¨¸ìŠ¤']]  # ê¸°ë³¸ í˜¸í™˜ ê·¸ë£¹
        
        logger.info(f"ğŸ­ íšŒì‚¬ ì—…ì¢… ê·¸ë£¹: {company_similar_groups}")
        
        from sqlalchemy import or_
        
        # ğŸ†• ì„œë¹„ìŠ¤ ìœ í˜•ë³„ ë¸”ë¡œê·¸ URL íŒ¨í„´ í•„í„°ë§
        # OPI: ê²°ì œ ì—°ë™/PG ê´€ë ¨
        # PS: í”Œë«í¼ ì •ì‚° (íŒŒíŠ¸ë„ˆ ì •ì‚°) - /ps_ ê²½ë¡œ
        # PRISM: ë©€í‹° ì˜¤í”ˆë§ˆì¼“ ì •ì‚° í†µí•© - /co- ê²½ë¡œ (Company ì‚¬ë¡€)
        service_url_patterns = {
            # OPI: ê²°ì œ ì—°ë™/PG ê´€ë ¨ + ê¸€ë¡œë²Œ ê²°ì œ
            'OPI': ['/opi_', '/payment_', '/pgcompare', '/onboarding', '/easypayment', '/billing-pay', '/case_', '/fitpet', '/v2-open', '/multi-pg', '/blue-garage', '/game', '/codemshop', '/global', '/woocommerce'],
            'PS': ['/ps_'],  # í”Œë«í¼ ì •ì‚° ì „ìš© (ps_odin, ps_news, ps_tech-lead) - âš ï¸ ì ˆëŒ€ OPI ë©”ì¼ì— ë„£ì§€ ë§ê²ƒ
            'PRISM': ['/co-', '/prism_', '/analytics']  # ë©€í‹° ì˜¤í”ˆë§ˆì¼“ ì •ì‚° (co-sabang, co-drg, co-skin1004)
        }
        
        # ë¸”ë¡œê·¸ ê²€ìƒ‰ (ìµœì‹ ìˆœ)
        query = db.session.query(BlogPost).order_by(BlogPost.created_at.desc())
        
        # ì„œë¹„ìŠ¤ ìœ í˜•ì´ ì§€ì •ë˜ë©´ í•´ë‹¹ íŒ¨í„´ë§Œ í•„í„°ë§
        if service_type and service_type.upper() in service_url_patterns:
            patterns = service_url_patterns[service_type.upper()]
            # OR ì¡°ê±´ìœ¼ë¡œ íŒ¨í„´ ë§¤ì¹­
            pattern_filters = [BlogPost.link.like(f'%{p}%') for p in patterns]
            query = query.filter(or_(*pattern_filters))
            logger.info(f"ğŸ” {service_type} ë¸”ë¡œê·¸ë§Œ ê²€ìƒ‰ (íŒ¨í„´: {patterns})")
        
        all_posts = query.limit(max_check).all()
        
        if not all_posts:
            logger.info(f"ğŸ“ ë¸”ë¡œê·¸ DBì— {service_type or 'ì „ì²´'} ë°ì´í„° ì—†ìŒ")
            return None
        
        best_match = None
        best_score = 0
        best_reason = ''
        industry_matched = False
        best_case_company = None  # ë¸”ë¡œê·¸ì— ì–¸ê¸‰ëœ ê³ ê°ì‚¬ëª…
        
        for post in all_posts:
            score = 0
            reasons = []
            this_industry_matched = False
            case_company_name = None  # ë¸”ë¡œê·¸ì— ì–¸ê¸‰ëœ ê³ ê°ì‚¬ëª…
            case_companies = []  # ë¸”ë¡œê·¸ì—ì„œ ì¶”ì¶œí•œ ê³ ê°ì‚¬ ë¦¬ìŠ¤íŠ¸ (ì´ˆê¸°í™” í•„ìˆ˜!)
            
            post_title_lower = (post.title or '').lower()
            blog_link_lower = (post.link or '').lower()
            
            # ğŸš« OPI ë©”ì¼ì— PS(í”Œë«í¼ ì •ì‚°) ë¸”ë¡œê·¸ ì ˆëŒ€ í¬í•¨ ê¸ˆì§€
            if service_type and service_type.upper() == 'OPI':
                ps_keywords = ['í”Œë«í¼ ì •ì‚°', 'íŒŒíŠ¸ë„ˆ ì •ì‚°', 'íŒŒíŠ¸ë„ˆì •ì‚°', 'ps_', 'ì •ì‚° ìë™í™”']
                if any(pk in post_title_lower or pk in blog_link_lower for pk in ps_keywords):
                    logger.info(f"ğŸš« OPI ë©”ì¼ì— PS ë¸”ë¡œê·¸ ì œì™¸: {post.title[:40]}...")
                    continue  # ì™„ì „íˆ ìŠ¤í‚µ
            
            # ğŸš« í˜¼ë‹¤ ë¸”ë¡œê·¸ëŠ” ìë™ì°¨/ì œì¡° ì—…ì¢…ì—ë§Œ (ì™„ì „ ìŠ¤í‚µ)
            if 'honda' in blog_link_lower:
                if not any(ind in matched_industries for ind in ['ìë™ì°¨', 'ì œì¡°', 'ë¬¼ë¥˜']):
                    logger.info(f"ğŸš« í˜¼ë‹¤ ë¸”ë¡œê·¸ ìŠ¤í‚µ: {post.title[:30]}... (ìë™ì°¨/ì œì¡° ì—…ì¢…ë§Œ)")
                    continue  # ì™„ì „íˆ ìŠ¤í‚µ
            
            post_text = f"{post.title} {post.summary} {post.content} {post.industry_tags} {post.keywords}".lower()
            
            # ğŸ†• AI ìš”ì•½ í•„ë“œë„ ë§¤ì¹­ì— í™œìš©
            ai_summary_text = f"{post.ai_summary or ''} {post.target_audience or ''} {post.pain_points_addressed or ''} {post.key_benefits or ''}".lower()
            full_text = f"{post_text} {ai_summary_text}"
            
            # ğŸ†• ë¸”ë¡œê·¸ ì—…ì¢… íŒŒì•… - DBì— ì €ì¥ëœ AI ìš”ì•½ ì •ë³´ ìš°ì„  ì‚¬ìš©!
            case_company_name = post.case_company or None
            blog_industries = []
            
            # 1ìˆœìœ„: DBì— ì €ì¥ëœ case_industry (AIê°€ ë¶„ì„í•œ ì—…ì¢… - ê°€ì¥ ì •í™•)
            if post.case_industry:
                # ì—¬ëŸ¬ ì—…ì¢…ì´ ì‰¼í‘œë¡œ êµ¬ë¶„ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ
                for ind in post.case_industry.split(','):
                    ind = ind.strip()
                    if ind and ind not in blog_industries:
                        blog_industries.append(ind)
                logger.debug(f"âœ… DB case_industry ì‚¬ìš©: {post.title[:20]}... â†’ {blog_industries}")
            
            # 2ìˆœìœ„: ë¸”ë¡œê·¸ ë³¸ë¬¸ì—ì„œ ê³ ê°ì‚¬ë¡€ ì¶”ì¶œ
            if not blog_industries:
                case_companies = extract_case_companies_from_blog(post.content or '', post.title or '')
                if case_companies:
                    for case in case_companies:
                        if case['industry'] not in blog_industries:
                            blog_industries.append(case['industry'])
                    case_company_name = case_companies[0]['company']
            
            # 3ìˆœìœ„: í‚¤ì›Œë“œ ê¸°ë°˜ (í´ë°±)
            if not blog_industries:
                for ind, keywords in industry_keywords.items():
                    for kw in keywords:
                        if kw in post_text:
                            if ind not in blog_industries:
                                blog_industries.append(ind)
                            break
            
            is_restricted_blog = 'honda' in blog_link_lower
            
            # ğŸ†• ì—…ì¢… ìœ ì‚¬ë„ ì²´í¬ - íšŒì‚¬ì™€ ë¸”ë¡œê·¸ ì—…ì¢…ì´ ë„ˆë¬´ ë‹¤ë¥´ë©´ í° í˜ë„í‹°
            # ì˜ˆ: ë·°í‹° íšŒì‚¬ â†’ ìë™ì°¨ ë¸”ë¡œê·¸ëŠ” ì‹ ë¹™ì„± ì—†ìŒ
            industry_compatible = False
            
            # ë¸”ë¡œê·¸ê°€ ë²”ìš©ì  ì—…ì¢…(í”Œë«í¼, ê¸€ë¡œë²Œ, ì´ì»¤ë¨¸ìŠ¤)ì´ë©´ ëª¨ë‘ì™€ í˜¸í™˜ (ë‹¨, ì œí•œëœ ë¸”ë¡œê·¸ ì œì™¸)
            universal_industries = ['í”Œë«í¼', 'ê¸€ë¡œë²Œ', 'ì´ì»¤ë¨¸ìŠ¤']
            if not is_restricted_blog and any(ind in blog_industries for ind in universal_industries):
                industry_compatible = True
            
            # íšŒì‚¬ ì—…ì¢… ê·¸ë£¹ê³¼ ë¸”ë¡œê·¸ ì—…ì¢…ì´ ê°™ì€ ê·¸ë£¹ì— ìˆìœ¼ë©´ í˜¸í™˜
            if not industry_compatible and company_similar_groups:
                for company_group in company_similar_groups:
                    if any(ind in blog_industries for ind in company_group):
                        industry_compatible = True
                        break
            
            # ë¸”ë¡œê·¸ ì—…ì¢…ì´ íŒŒì•… ì•ˆ ë˜ë©´ ì¼ë‹¨ í˜¸í™˜ìœ¼ë¡œ ì²˜ë¦¬
            if not blog_industries:
                industry_compatible = True
            
            # ì—…ì¢…ì´ ë„ˆë¬´ ë‹¤ë¥´ë©´ -50ì  í˜ë„í‹° (ì˜ë„ ë§¤ì¹­ 100ì ë„ ë¬´ë ¥í™”)
            if not industry_compatible and blog_industries:
                score -= 50
                logger.debug(f"âš ï¸ ì—…ì¢… ë¶ˆì¼ì¹˜ í˜ë„í‹°: {post.title[:30]}... (íšŒì‚¬: {matched_industries}, ë¸”ë¡œê·¸: {blog_industries})")
            
            # ğŸ¯ ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ íŒŒì•…ëœ ì˜ë„ì™€ ë¸”ë¡œê·¸ ë§¤ì¹­ (ìµœìš°ì„ !)
            title_lower = (post.title or '').lower()
            for intent_name in detected_intents:
                intent_info = intent_scenarios.get(intent_name, {})
                blog_kws = intent_info.get('blog_keywords', [])
                intent_score = intent_info.get('score', 30)
                
                # ë¸”ë¡œê·¸ ì œëª©ì— ì˜ë„ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ìµœê³  ì ìˆ˜
                if any(bk in title_lower for bk in blog_kws):
                    score += intent_score
                    this_industry_matched = True  # ì—…ì¢… ë¶ˆì¼ì¹˜ íŒ¨ë„í‹° ë°©ì§€
                    reasons.insert(0, f"ğŸ“° {intent_name} ê´€ë ¨ ì „ë¬¸ ì‚¬ë¡€")
                    break
                # ë¸”ë¡œê·¸ ë³¸ë¬¸ì— ì˜ë„ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¤‘ê°„ ì ìˆ˜
                elif any(bk in post_text for bk in blog_kws):
                    score += intent_score // 2
                    reasons.insert(0, f"{intent_name} ê´€ë ¨")
            
            # ï¿½ AI ìš”ì•½ ê¸°ë°˜ íƒ€ê²Ÿ ê³ ê° ë§¤ì¹­ (ì •í™•ë„ í–¥ìƒ!)
            target_audience = (post.target_audience or '').lower()
            pain_points = (post.pain_points_addressed or '').lower()
            
            # ì˜ë„ í‚¤ì›Œë“œì™€ target_audience ë§¤ì¹­
            for intent_name in detected_intents:
                intent_info = intent_scenarios.get(intent_name, {})
                intent_keywords = intent_info.get('keywords', [])
                
                # target_audienceì— ì˜ë„ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¶”ê°€ ì ìˆ˜
                if any(kw in target_audience for kw in intent_keywords):
                    score += 30
                    if f"íƒ€ê²Ÿë§¤ì¹­" not in str(reasons):
                        reasons.append(f"ğŸ¯ íƒ€ê²Ÿë§¤ì¹­: {intent_name}")
                    break
            
            # íšŒì‚¬ì˜ pain pointì™€ ë¸”ë¡œê·¸ì˜ í•´ê²° ë¬¸ì œ ë§¤ì¹­
            company_pain_keywords = []
            for pain, kws in {
                'pgê´€ë¦¬': ['pg', 'ë³µìˆ˜', 'ë©€í‹°', 'ê´€ë¦¬'],
                'ìˆ˜ìˆ˜ë£Œ': ['ìˆ˜ìˆ˜ë£Œ', 'ë¹„ìš©', 'ì ˆê°'],
                'ì •ì‚°': ['ì •ì‚°', 'ëŒ€ì‚¬', 'ë§ˆê°'],
                'ê¸€ë¡œë²Œ': ['í•´ì™¸', 'ê¸€ë¡œë²Œ', 'í™˜ìœ¨', 'í¬ë¡œìŠ¤ë³´ë”'],
            }.items():
                if any(kw in all_text for kw in kws):
                    company_pain_keywords.extend(kws)
            
            if company_pain_keywords and any(kw in pain_points for kw in company_pain_keywords):
                score += 20
                if "ë¬¸ì œí•´ê²°ë§¤ì¹­" not in str(reasons):
                    reasons.append("ğŸ’¡ ë¬¸ì œí•´ê²°ë§¤ì¹­")
            
            # ï¿½ï¿½ ê²½ìŸì‚¬ ë§¤ì¹­ (ìµœê³  ì ìˆ˜ - ê°€ì¥ ì„¤ë“ë ¥ ìˆìŒ!)
            competitor_matched = False
            if competitor_list:
                for comp in competitor_list:
                    if comp in post_text:
                        score += 25  # ê²½ìŸì‚¬ ì–¸ê¸‰ ì‹œ ìµœê³  ì ìˆ˜
                        competitor_matched = True
                        reasons.insert(0, f"ê²½ìŸì‚¬ '{comp}' ì‚¬ë¡€")
                        logger.info(f"ğŸ† ê²½ìŸì‚¬ ë§¤ì¹­! '{comp}' in blog: {post.title[:30]}...")
                        break
                    # ë¸”ë¡œê·¸ì—ì„œ ì¶”ì¶œí•œ ê³ ê°ì‚¬ê°€ ê²½ìŸì‚¬ì¸ ê²½ìš°
                    if case_companies:
                        for case in case_companies:
                            if comp in case['company'].lower():
                                score += 25
                                competitor_matched = True
                                reasons.insert(0, f"ê²½ìŸì‚¬ '{case['company']}' ì‚¬ë¡€")
                                case_company_name = case['company']
                                break
                        if competitor_matched:
                            break
            
            # ì‚°ì—… ë§¤ì¹­ ì ìˆ˜ (ë†’ì€ ê°€ì¤‘ì¹˜)
            for ind in matched_industries:
                if ind in blog_industries:
                    score += 15  # ì •í™•íˆ ê°™ì€ ì—…ì¢…
                    this_industry_matched = True
                    # ğŸ†• ê³ ê°ì‚¬ëª…ì´ ìˆìœ¼ë©´ ë” êµ¬ì²´ì ì¸ ì´ìœ  í‘œì‹œ
                    if case_company_name and f"{ind}" not in str(reasons):
                        reasons.append(f"{case_company_name}({ind})")
                    elif f"{ind} ì—…ì¢…" not in [r for r in reasons]:
                        reasons.append(f"{ind} ì—…ì¢… ì‚¬ë¡€")
                else:
                    # í‚¤ì›Œë“œë¡œ ë¶€ë¶„ ë§¤ì¹­
                    for kw in industry_keywords.get(ind, []):
                        if kw in post_text:
                            score += 5
                            this_industry_matched = True
                            if f"{ind}" not in str(reasons):
                                reasons.append(f"{ind} ê´€ë ¨")
                            break
            
            # í˜œíƒ ë§¤ì¹­ ì ìˆ˜
            for benefit in matched_benefits:
                for kw in benefit_keywords.get(benefit, []):
                    if kw in post_text:
                        score += 5
                        if benefit not in str(reasons):
                            reasons.append(f"{benefit}")
                        break
            
            # ì¼ë°˜ í˜œíƒ í‚¤ì›Œë“œ (íšŒì‚¬ ë§¤ì¹­ ì—†ì–´ë„)
            general_benefits = ['ìˆ˜ìˆ˜ë£Œ', 'ì ˆê°', 'ìë™í™”', 'íš¨ìœ¨', 'ì„±ê³µì‚¬ë¡€', 'ë„ì…ì‚¬ë¡€']
            for gb in general_benefits:
                if gb in post_text:
                    score += 1
            
            # ğŸ†• ì—…ì¢… ë§¤ì¹­ì´ ìˆëŠ” ë¸”ë¡œê·¸ ìš°ì„  (ì—…ì¢… ë§¤ì¹­ ì—†ìœ¼ë©´ ì ìˆ˜ ê°ì )
            if not this_industry_matched and matched_industries:
                score = score // 2  # ì—…ì¢… ë¶ˆì¼ì¹˜ ì‹œ ì ìˆ˜ ë°˜ê°
            
            # URLì´ ìœ íš¨í•œì§€ í™•ì¸
            if score > best_score and post.link:
                best_score = score
                best_match = post
                best_reason = ', '.join(reasons[:2]) if reasons else 'í¬íŠ¸ì› ë„ì… íš¨ê³¼'
                industry_matched = this_industry_matched
                best_case_company = case_company_name  # ê³ ê°ì‚¬ëª… ì €ì¥
        
        # ğŸ†• ìµœì†Œ ì ìˆ˜ ê¸°ì¤€ ì™„í™”: ë¸”ë¡œê·¸ ì–¸ê¸‰ì„ ë” ì ê·¹ì ìœ¼ë¡œ í•˜ê¸° ìœ„í•´
        # ì—…ì¢… ë§¤ì¹­ ìˆìœ¼ë©´ 3ì , ì—†ìœ¼ë©´ 5ì  ì´ìƒì´ë©´ OK
        min_score = 3 if industry_matched else 5
        
        if best_match and best_score >= min_score:
            logger.info(f"âœ… ë¸”ë¡œê·¸ ì„ íƒ: {best_match.title[:40]}... (ì ìˆ˜: {best_score}, ì´ìœ : {best_reason}, ì—…ì¢…ë§¤ì¹­: {industry_matched}, ê³ ê°ì‚¬: {best_case_company})")
            return {
                'title': best_match.title,
                'link': best_match.link,
                'summary': best_match.summary[:200] if best_match.summary else '',
                'match_reason': best_reason,
                'industry_matched': industry_matched,
                'case_company': best_case_company  # ë¸”ë¡œê·¸ì— ì–¸ê¸‰ëœ ê³ ê°ì‚¬ëª…
            }
        else:
            logger.info(f"ğŸ“ ì í•©í•œ ë¸”ë¡œê·¸ ì—†ìŒ (ìµœê³  ì ìˆ˜: {best_score}, ìµœì†Œ ê¸°ì¤€: {min_score})")
            return None
            
    except Exception as e:
        logger.error(f"ë¸”ë¡œê·¸ ì„ íƒ ì˜¤ë¥˜: {str(e)}")
        return None


def format_blog_mention_for_email(blog_info, company_name=''):
    """
    ì´ë©”ì¼ì— ì‚½ì…í•  ë¸”ë¡œê·¸ ì–¸ê¸‰ ë¬¸êµ¬ ìƒì„±
    
    "3,000ì—¬ê°œ ê³ ê°ì‚¬ê°€..." ëŒ€ì‹  ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¬¸êµ¬
    
    Args:
        blog_info: get_best_blog_for_email_mention() ê²°ê³¼
        company_name: íšŒì‚¬ëª…
    
    Returns:
        dict: {
            'mention_text': ë³¸ë¬¸ì— ì‚½ì…í•  í…ìŠ¤íŠ¸,
            'blog_link': ë¸”ë¡œê·¸ ë§í¬,
            'blog_title': ë¸”ë¡œê·¸ ì œëª©
        }
    """
    if not blog_info:
        return None
    
    title = blog_info.get('title', '')
    link = blog_info.get('link', '')
    reason = blog_info.get('match_reason', '')
    
    # ë³¸ë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ì‚½ì…í•  ë¬¸êµ¬
    mention_text = f"""
ì‹¤ì œë¡œ {reason}ë¥¼ ê³ ë¯¼í•˜ì…¨ë˜ ê³ ê°ì‚¬ì—ì„œ í¬íŠ¸ì› ë„ì… í›„ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ìœ¼ì…¨ëŠ”ë°ìš”,
ìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ ê¸€ì—ì„œ í™•ì¸í•´ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ‘‰ [{title}]({link})
"""
    
    return {
        'mention_text': mention_text.strip(),
        'blog_link': link,
        'blog_title': title,
        'match_reason': reason
    }


def format_relevant_blog_for_email(blog_posts, company_name='', service_type=''):
    """
    ì—…ì¢…ë³„ ê´€ë ¨ ë¸”ë¡œê·¸ ê¸€ì„ RAG ë°©ì‹ìœ¼ë¡œ í¬ë§·íŒ…
    
    Args:
        blog_posts: ë¸”ë¡œê·¸ ê¸€ ë¦¬ìŠ¤íŠ¸
        company_name: íšŒì‚¬ëª…
        service_type: ì„œë¹„ìŠ¤ íƒ€ì…
    
    Returns:
        str: í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸
    """
    if not blog_posts:
        return ''
    
    service_label = service_type if service_type else 'í¬íŠ¸ì›'
    
    content = f"\n\n**ğŸ“š {service_label} ê´€ë ¨ ì°¸ê³  ì •ë³´ (RAG - Pain Point ë§¤ì¹­ ì‚¬ë¡€ ìš°ì„ !):**\n\n"
    content += "âš ï¸ **ì¤‘ìš” ì§€ì¹¨**: ì•„ë˜ ì •ë³´ëŠ” ì´ë©”ì¼ ë³¸ë¬¸ì˜ ì„¤ë“ë ¥ì„ ë†’ì´ê¸° ìœ„í•œ ì°¸ê³  ìë£Œì…ë‹ˆë‹¤.\n"
    content += "- ë¸”ë¡œê·¸ ê¸€ì„ ì§ì ‘ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš” (\"ìµœê·¼ í¬íŠ¸ì› ë¸”ë¡œê·¸ì—ì„œ...\" âŒ)\n"
    content += "- **ì•„ë˜ ë¸”ë¡œê·¸ëŠ” {company_name}ì˜ Pain Pointì™€ ìœ ì‚¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•œ ê¸°ì¡´ ê³ ê° ì‚¬ë¡€ì…ë‹ˆë‹¤**\n"
    content += "- **ì°¸ê³ ìë£Œ 1ë²ˆì´ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì‚¬ë¡€**ì´ë¯€ë¡œ ìš°ì„  í™œìš©í•˜ì„¸ìš”\n"
    content += "- ì •ë³´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í™œìš©í•˜ì—¬ \"{company_name}ë‹˜ë„ ì´ëŸ° ë¬¸ì œ ê²ªìœ¼ì‹œì£ ?\"ë¼ëŠ” ê³µê°ëŒ€ í˜•ì„±\n"
    content += "- ìˆ˜ì¹˜, íŠ¸ë Œë“œ, ì‚¬ë¡€ ë“±ì„ ìì‹ ì˜ ë§ë¡œ ë…¹ì—¬ì„œ ì‚¬ìš©í•˜ì„¸ìš”\n\n"
    content += "---\n\n"
    
    for i, post in enumerate(blog_posts[:3], 1):
        content += f"**ì°¸ê³ ìë£Œ {i}:**\n"
        content += f"ì£¼ì œ: {post['title']}\n"
        content += f"ğŸ”— **ì›ë³¸ ë§í¬ (ì´ë©”ì¼ ì¶œì²˜ë¡œ ì‚¬ìš© ì‹œ ì´ URLì„ ì •í™•íˆ ë³µì‚¬)**: {post.get('link', '')}\n\n"
        
        summary = post.get('summary', '')
        full_content = post.get('content', '')
        
        if summary:
            content += f"í•µì‹¬ ë‚´ìš©:\n{summary}\n\n"
        
        if full_content and len(full_content) > len(summary):
            additional = full_content[len(summary):min(len(summary)+300, len(full_content))]
            content += f"ì¶”ê°€ ì •ë³´:\n{additional}...\n\n"
        
        content += "---\n\n"
    
    content += f"ğŸ’¡ **Pain Point ë§¤ì¹­ ì‚¬ë¡€ í™œìš©ë²•**: \n"
    content += f"- ìœ„ ë¸”ë¡œê·¸ëŠ” {company_name}ì™€ ìœ ì‚¬í•œ Pain Pointë¥¼ ê²ªì€ ê¸°ì¡´ ê³ ê°ì˜ ì„±ê³µ ì‚¬ë¡€ì…ë‹ˆë‹¤\n"
    content += f"- ì´ë©”ì¼ì—ì„œ \"{company_name}ë‹˜ë„ ì´ëŸ° ì–´ë ¤ì›€ ê²ªê³  ê³„ì‹œì§€ ì•Šë‚˜ìš”?\"ë¼ëŠ” ê³µê°ìœ¼ë¡œ ì‹œì‘\n"
    content += f"- ê¸°ì¡´ ê³ ê°ì´ ì–´ë–»ê²Œ ë¬¸ì œë¥¼ í•´ê²°í–ˆëŠ”ì§€ êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ í•¨ê»˜ ì–¸ê¸‰\n"
    content += f"- ì˜ˆ: \"ìœ ì‚¬í•œ ì—…ì¢…ì˜ Xì‚¬ëŠ” í¬íŠ¸ì› ë„ì… í›„ Y% ê°œì„  íš¨ê³¼ë¥¼ ë³´ì•˜ìŠµë‹ˆë‹¤\"\n"
    content += f"- ì¶œì²˜ë¥¼ ëª…ì‹œí•  ê²½ìš° ì´ë©”ì¼ í•˜ë‹¨ì— [ì°¸ê³ ] í˜•ì‹ìœ¼ë¡œë§Œ í‘œê¸°\n"
    
    return content

def get_service_knowledge(service_type=''):
    """
    ì„œë¹„ìŠ¤ ì†Œê°œì„œì™€ ë¸”ë¡œê·¸ ì „ì²´ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ RAG ì§€ì‹ë² ì´ìŠ¤ ìƒì„±
    
    Args:
        service_type: 'OPI', 'PS', 'PRISM'
    
    Returns:
        str: í†µí•©ëœ ì§€ì‹ë² ì´ìŠ¤ í…ìŠ¤íŠ¸
    """
    knowledge = ""
    
    # 1. ì„œë¹„ìŠ¤ ì†Œê°œì„œ ë¡œë“œ
    service_files = {
        'OPI': 'opi_service_info.txt',
        'PRISM': 'prism_service_info.txt',
        'PS': 'ps_service_info.txt'
    }
    
    service_names = {
        'OPI': 'One Payment Infra (OPI)',
        'PRISM': 'ë©€í‹° ì˜¤í”ˆë§ˆì¼“ ì •ì‚° í†µí•© ì†”ë£¨ì…˜ (Prism)',
        'PS': 'í”Œë«í¼ ì •ì‚° ìë™í™”'
    }
    
    if service_type in service_files:
        try:
            with open(service_files[service_type], 'r', encoding='utf-8') as f:
                service_doc = f.read()
            knowledge += f"\n\n**ğŸ“– {service_names[service_type]} ì„œë¹„ìŠ¤ ì†Œê°œ:**\n\n"
            knowledge += f"{service_doc[:3000]}...\n\n"
            logger.info(f"âœ… {service_type} ì„œë¹„ìŠ¤ ì†Œê°œì„œ ë¡œë“œ ì™„ë£Œ")
        except:
            logger.warning(f"âš ï¸ {service_type} ì„œë¹„ìŠ¤ ì†Œê°œì„œ íŒŒì¼ ì—†ìŒ")
    
    # 2. ë¸”ë¡œê·¸ ì „ì²´ ìš”ì•½ (PostgreSQLì—ì„œ ì¡°íšŒ)
    try:
        db = get_db()
        BlogPost = get_blog_post_model()
        
        posts_query = db.session.query(BlogPost).filter_by(category=service_type).order_by(BlogPost.created_at.desc()).all()
        
        if posts_query:
            knowledge += f"\n\n**ğŸ“š {service_type} ê´€ë ¨ ë¸”ë¡œê·¸ ì¸ì‚¬ì´íŠ¸ ({len(posts_query)}ê°œ ê¸€):**\n\n"
            knowledge += f"ë‹¤ìŒì€ í¬íŠ¸ì› ê³µì‹ ë¸”ë¡œê·¸ì—ì„œ {service_type} ê´€ë ¨ {len(posts_query)}ê°œ ê¸€ì˜ í•µì‹¬ ë‚´ìš©ì…ë‹ˆë‹¤.\n"
            knowledge += "ì´ ì •ë³´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì—…ê³„ íŠ¸ë Œë“œ, Pain Point, ì‚¬ë¡€ ë“±ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•˜ì„¸ìš”.\n\n"
            
            # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            all_keywords = []
            for post in posts_query:
                if post.keywords:
                    keywords = post.keywords.split(',')
                    all_keywords.extend(keywords)
            
            if all_keywords:
                keyword_freq = Counter(all_keywords)
                top_keywords = [k for k, v in keyword_freq.most_common(10)]
                knowledge += f"**ì£¼ìš” í‚¤ì›Œë“œ**: {', '.join(top_keywords)}\n\n"
            
            # ëŒ€í‘œ ê¸€ 5ê°œ ìš”ì•½
            knowledge += f"**ëŒ€í‘œ ì¸ì‚¬ì´íŠ¸:**\n\n"
            for i, post in enumerate(posts_query[:5], 1):
                knowledge += f"{i}. {post.title}\n"
                knowledge += f"   ğŸ”— **ì›ë³¸ ë§í¬**: {post.link}\n"
                if post.summary:
                    knowledge += f"   â†’ {post.summary[:150]}...\n\n"
            
            logger.info(f"âœ… {service_type} ë¸”ë¡œê·¸ {len(posts_query)}ê°œ ìš”ì•½ ì™„ë£Œ (PostgreSQL)")
            
    except Exception as e:
        logger.error(f"ë¸”ë¡œê·¸ ìš”ì•½ ì˜¤ë¥˜: {str(e)}")
    
    # 3. RAG í™œìš© ì§€ì¹¨
    knowledge += f"\n\n**ğŸ’¡ ì§€ì‹ í™œìš© ê°€ì´ë“œ:**\n"
    knowledge += "- ìœ„ ì„œë¹„ìŠ¤ ì†Œê°œì„œì™€ ë¸”ë¡œê·¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ê¹Šì´ ì´í•´í•˜ê³  í™œìš©í•˜ì„¸ìš”\n"
    knowledge += "- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ê¸°ëŠ¥, íš¨ê³¼ë¥¼ ì •í™•í•˜ê²Œ ì–¸ê¸‰í•˜ì„¸ìš”\n"
    knowledge += "- ì—…ê³„ íŠ¸ë Œë“œë‚˜ Pain PointëŠ” 'ì—…ê³„ì—ì„œëŠ”...', 'ë§ì€ ê¸°ì—…ë“¤ì´...' í˜•íƒœë¡œ ìì—°ìŠ¤ëŸ½ê²Œ\n"
    knowledge += "- ê²½ìŸë ¥ ìˆëŠ” ì°¨ë³„ì ê³¼ í•µì‹¬ ê°€ì¹˜ë¥¼ ëª…í™•íˆ ì „ë‹¬í•˜ì„¸ìš”\n"
    knowledge += f"- {service_type} ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì „ë¬¸ì„±ê³¼ ì‹ ë¢°ì„±ì„ ë³´ì—¬ì£¼ì„¸ìš”\n"
    
    return knowledge

def get_existing_blog_links():
    """
    DBì— ì´ë¯¸ ì €ì¥ëœ ë¸”ë¡œê·¸ ë§í¬ ëª©ë¡ ì¡°íšŒ
    
    Returns:
        set: ê¸°ì¡´ ë¸”ë¡œê·¸ ë§í¬ ì§‘í•©
    """
    try:
        db = get_db()
        BlogPost = get_blog_post_model()
        
        # ëª¨ë“  ë§í¬ ì¡°íšŒ
        posts = db.session.query(BlogPost.link).all()
        existing_links = {post.link for post in posts if post.link}
        
        logger.info(f"ğŸ“‹ DBì— ì €ì¥ëœ ë¸”ë¡œê·¸: {len(existing_links)}ê°œ")
        return existing_links
        
    except Exception as e:
        logger.error(f"ê¸°ì¡´ ë§í¬ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return set()

def check_for_new_posts(category_url, existing_links, max_check_pages=2):
    """
    ë¸”ë¡œê·¸ ì¹´í…Œê³ ë¦¬ì—ì„œ ìƒˆë¡œìš´ í¬ìŠ¤íŠ¸ë§Œ í™•ì¸
    
    Args:
        category_url: ì¹´í…Œê³ ë¦¬ URL
        existing_links: ê¸°ì¡´ ë¸”ë¡œê·¸ ë§í¬ ì§‘í•©
        max_check_pages: í™•ì¸í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ 2í˜ì´ì§€)
    
    Returns:
        list: ìƒˆë¡œìš´ í¬ìŠ¤íŠ¸ ë§í¬ ëª©ë¡
    """
    try:
        from bs4 import BeautifulSoup
        import requests
        
        new_post_links = []
        
        for page in range(1, max_check_pages + 1):
            page_url = f"{category_url}&page={page}" if page > 1 else category_url
            
            response = requests.get(page_url, timeout=10)
            if response.status_code != 200:
                logger.warning(f"í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {page_url}")
                break
            
            soup = BeautifulSoup(response.text, 'html.parser')
            articles = soup.find_all('article', class_='post')
            
            if not articles:
                logger.info(f"ë” ì´ìƒ ê¸€ì´ ì—†ìŒ (í˜ì´ì§€ {page})")
                break
            
            found_existing = False
            for article in articles:
                link_tag = article.find('a', href=True)
                if link_tag:
                    link = f"https://blog.portone.io{link_tag['href']}"
                    
                    # ê¸°ì¡´ DBì— ì—†ëŠ” ìƒˆë¡œìš´ ê¸€ë§Œ ì¶”ê°€
                    if link not in existing_links:
                        new_post_links.append(link)
                    else:
                        found_existing = True
            
            # ê¸°ì¡´ ê¸€ì„ ë°œê²¬í•˜ë©´ ë” ì´ìƒ í™•ì¸ ë¶ˆí•„ìš”
            if found_existing:
                logger.info(f"ê¸°ì¡´ ê¸€ ë°œê²¬ - {page}í˜ì´ì§€ì—ì„œ í™•ì¸ ì¤‘ë‹¨")
                break
        
        return new_post_links
        
    except Exception as e:
        logger.error(f"ìƒˆ í¬ìŠ¤íŠ¸ í™•ì¸ ì˜¤ë¥˜: {str(e)}")
        return []


def analyze_news_for_blog_recommendation(news_content, company_name, industry='', research_data=None):
    """
    ğŸ†• ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ AIë¡œ ë¶„ì„í•˜ì—¬ ê¸°ì—…ì˜ ì˜ë„, ì˜ˆìƒ ì–´ë ¤ì›€, í•„ìš” ì •ë³´ë¥¼ ì¶”ì¶œ
    
    Args:
        news_content: ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš© (ë¬¸ìì—´ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬)
        company_name: íšŒì‚¬ëª…
        industry: ì—…ì¢…
        research_data: Perplexity ì¡°ì‚¬ ê²°ê³¼
    
    Returns:
        dict: {
            'business_intent': ê¸°ì—…ì´ í•˜ë ¤ëŠ” ê²ƒ (ì˜ˆ: "í•´ì™¸ ì§„ì¶œ", "êµ¬ë… ì„œë¹„ìŠ¤ ëŸ°ì¹­"),
            'expected_challenges': ì˜ˆìƒë˜ëŠ” ì–´ë ¤ì›€ë“¤ (ë¦¬ìŠ¤íŠ¸),
            'information_needs': í•„ìš”í•  ê²ƒ ê°™ì€ ì •ë³´ (ë¦¬ìŠ¤íŠ¸),
            'recommended_topics': ì¶”ì²œ ë¸”ë¡œê·¸ ì£¼ì œ (ë¦¬ìŠ¤íŠ¸),
            'urgency_level': ê¸´ê¸‰ë„ (high/medium/low),
            'confidence': ë¶„ì„ ì‹ ë¢°ë„ (0-1)
        }
    """
    import os
    import json
    
    try:
        import google.generativeai as genai
        
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            logger.warning("Gemini API í‚¤ ì—†ìŒ - ë‰´ìŠ¤ ë¶„ì„ ìŠ¤í‚µ")
            return None
        
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash')
        
        # ë‰´ìŠ¤ ë‚´ìš© ì¶”ì¶œ
        if isinstance(news_content, dict):
            news_text = f"ì œëª©: {news_content.get('title', '')}\në‚´ìš©: {news_content.get('content', '')}"
        else:
            news_text = str(news_content)
        
        # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ - Perplexity ë¦¬ì„œì¹˜ ë°ì´í„° ì ê·¹ í™œìš©
        additional_context = ""
        if research_data:
            research_parts = []
            if research_data.get('summary'):
                research_parts.append(f"- ì¡°ì‚¬ ìš”ì•½: {research_data.get('summary', '')[:500]}")
            if research_data.get('company_info'):
                research_parts.append(f"- íšŒì‚¬ ì •ë³´: {research_data.get('company_info', '')[:400]}")
            if research_data.get('key_findings'):
                findings = research_data.get('key_findings', [])
                if isinstance(findings, list):
                    research_parts.append(f"- ì£¼ìš” ë°œê²¬: {', '.join(findings[:5])}")
                else:
                    research_parts.append(f"- ì£¼ìš” ë°œê²¬: {str(findings)[:300]}")
            if research_data.get('pain_points'):
                pains = research_data.get('pain_points', [])
                if isinstance(pains, list):
                    research_parts.append(f"- ì£¼ìš” ê³ ì¶©/ì–´ë ¤ì›€: {', '.join(pains[:5])}")
                else:
                    research_parts.append(f"- ì£¼ìš” ê³ ì¶©/ì–´ë ¤ì›€: {str(pains)[:300]}")
            if research_data.get('business_model'):
                research_parts.append(f"- ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸: {research_data.get('business_model', '')}")
            if research_data.get('competitors'):
                comps = research_data.get('competitors', [])
                if isinstance(comps, list):
                    research_parts.append(f"- ê²½ìŸì‚¬: {', '.join(comps[:5])}")
            if research_data.get('recent_news'):
                research_parts.append(f"- ìµœê·¼ ë‰´ìŠ¤: {str(research_data.get('recent_news', ''))[:300]}")
            
            if research_parts:
                additional_context = "\n**Perplexity ì¡°ì‚¬ ê²°ê³¼ (AIê°€ ì¡°ì‚¬í•œ íšŒì‚¬ ì •ë³´):**\n" + "\n".join(research_parts)
        
        prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ì™€ íšŒì‚¬ ì •ë³´ë¥¼ ë¶„ì„í•´ì„œ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

**íšŒì‚¬ëª…:** {company_name}
**ì—…ì¢…:** {industry}

**ë‰´ìŠ¤ ê¸°ì‚¬:**
{news_text[:3000]}

{additional_context}

**ë¶„ì„ ìš”ì²­:**
ì´ íšŒì‚¬ê°€ ë‰´ìŠ¤ì—ì„œ ì–¸ê¸‰ëœ í™œë™ì„ ìˆ˜í–‰í•  ë•Œ, ê²°ì œ/ì •ì‚° ê´€ë ¨í•´ì„œ ì–´ë–¤ ë„ì›€ì´ í•„ìš”í• ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.

**JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€ (ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ ì¤€ìˆ˜):**
```json
{{
    "business_intent": "ì´ íšŒì‚¬ê°€ í•˜ë ¤ëŠ” í•µì‹¬ í™œë™ (ì˜ˆ: 'í•´ì™¸ ì‹œì¥ ì§„ì¶œ', 'êµ¬ë… ì„œë¹„ìŠ¤ ëŸ°ì¹­', 'ì‹ ê·œ ì´ì»¤ë¨¸ìŠ¤ í”Œë«í¼ êµ¬ì¶•')",
    "expected_challenges": [
        "ì˜ˆìƒë˜ëŠ” ê²°ì œ/ì •ì‚° ê´€ë ¨ ì–´ë ¤ì›€ 1",
        "ì˜ˆìƒë˜ëŠ” ê²°ì œ/ì •ì‚° ê´€ë ¨ ì–´ë ¤ì›€ 2"
    ],
    "information_needs": [
        "ì´ íšŒì‚¬ê°€ í•„ìš”ë¡œ í•  ì •ë³´ 1 (ì˜ˆ: 'í•´ì™¸ ê²°ì œ ìˆ˜ë‹¨ ì—°ë™ ë°©ë²•')",
        "ì´ íšŒì‚¬ê°€ í•„ìš”ë¡œ í•  ì •ë³´ 2"
    ],
    "recommended_topics": [
        "ì¶”ì²œí•  ë¸”ë¡œê·¸ ì£¼ì œ 1 (ì˜ˆ: 'ê¸€ë¡œë²Œ ê²°ì œ ì„±ê³µ ì‚¬ë¡€')",
        "ì¶”ì²œí•  ë¸”ë¡œê·¸ ì£¼ì œ 2"
    ],
    "portone_solution": "ê°€ì¥ ì í•©í•œ í¬íŠ¸ì› ì†”ë£¨ì…˜ (OPI/PS/PRISM ì¤‘ ì„ íƒ)",
    "urgency_level": "high/medium/low (ë‰´ìŠ¤ ë‚´ìš© ê¸°ë°˜ ê¸´ê¸‰ë„)",
    "confidence": 0.8
}}
```

**ì£¼ì˜ì‚¬í•­:**
- ê²°ì œ, ì •ì‚°, PG, ìˆ˜ìˆ˜ë£Œ, í•´ì™¸ê²°ì œ ë“± PortOneì´ ë„ìš¸ ìˆ˜ ìˆëŠ” ì˜ì—­ì— ì§‘ì¤‘
- ë‰´ìŠ¤ì—ì„œ ëª…í™•í•œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì—…ì¢… íŠ¹ì„± ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ 
- ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥ (ì„¤ëª… í…ìŠ¤íŠ¸ ì—†ì´)
"""
        
        response = model.generate_content(prompt)
        
        if response and response.text:
            # JSON íŒŒì‹±
            result_text = response.text.strip()
            
            # ```json ... ``` ë¸”ë¡ ì¶”ì¶œ
            if '```json' in result_text:
                result_text = result_text.split('```json')[1].split('```')[0].strip()
            elif '```' in result_text:
                result_text = result_text.split('```')[1].split('```')[0].strip()
            
            try:
                analysis = json.loads(result_text)
                logger.info(f"âœ… {company_name} ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ: ì˜ë„={analysis.get('business_intent', '')[:30]}")
                return analysis
            except json.JSONDecodeError as e:
                logger.warning(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                # ê¸°ë³¸ê°’ ë°˜í™˜
                return {
                    'business_intent': 'ì‚¬ì—… í™•ì¥',
                    'expected_challenges': ['ê²°ì œ ì‹œìŠ¤í…œ ë³µì¡ì„±', 'ì •ì‚° ê´€ë¦¬ ì–´ë ¤ì›€'],
                    'information_needs': ['ê²°ì œ ì—°ë™ ê°€ì´ë“œ', 'ì •ì‚° ìë™í™” ì‚¬ë¡€'],
                    'recommended_topics': ['ê²°ì œ ì¸í”„ë¼ êµ¬ì¶•', 'ì •ì‚° íš¨ìœ¨í™”'],
                    'portone_solution': 'OPI',
                    'urgency_level': 'medium',
                    'confidence': 0.5
                }
        
        return None
        
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return None


def get_smart_blog_recommendation(company_info, research_data=None, news_analysis=None, service_type=None, max_blogs=3):
    """
    ğŸ†• AI ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ì í•©í•œ ë¸”ë¡œê·¸ë¥¼ ì¶”ì²œ
    
    ê¸°ì¡´ get_best_blog_for_email_mention()ì˜ í‚¤ì›Œë“œ ë§¤ì¹­ + AI ë¶„ì„ ê²°ê³¼ ê²°í•©
    
    Args:
        company_info: íšŒì‚¬ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        research_data: Perplexity ì¡°ì‚¬ ê²°ê³¼
        news_analysis: analyze_news_for_blog_recommendation() ê²°ê³¼
        service_type: ì„œë¹„ìŠ¤ ìœ í˜• (OPI/PS/PRISM)
        max_blogs: ìµœëŒ€ ì¶”ì²œ ë¸”ë¡œê·¸ ìˆ˜
    
    Returns:
        dict: {
            'primary_blog': ë©”ì¸ ì¶”ì²œ ë¸”ë¡œê·¸ (ì´ë©”ì¼ì— í¬í•¨),
            'supporting_blogs': ì°¸ê³ ìš© ë¸”ë¡œê·¸ ë¦¬ìŠ¤íŠ¸,
            'recommendation_reason': AIê°€ ì¶”ì²œí•˜ëŠ” ì´ìœ ,
            'email_mention_text': ì´ë©”ì¼ì— ì‚½ì…í•  ë¬¸êµ¬
        }
    """
    import os
    import json
    
    try:
        db = get_db()
        BlogPost = get_blog_post_model()
        
        company_name = company_info.get('company_name', '') or company_info.get('íšŒì‚¬ëª…', '')
        industry = company_info.get('industry', '')
        
        # ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ í•¨ìˆ˜ë¡œ í´ë°±
        if not news_analysis:
            logger.info(f"ğŸ“ {company_name}: ë‰´ìŠ¤ ë¶„ì„ ì—†ìŒ - ê¸°ì¡´ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ í´ë°±")
            basic_blog = get_best_blog_for_email_mention(company_info, research_data, service_type=service_type)
            if basic_blog:
                return {
                    'primary_blog': basic_blog,
                    'supporting_blogs': [],
                    'recommendation_reason': basic_blog.get('match_reason', 'ì—…ì¢… ê´€ë ¨ ì‚¬ë¡€'),
                    'email_mention_text': _generate_email_mention_text(basic_blog, company_name)
                }
            return None
        
        # ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ì—ì„œ ì¶”ì²œ ì£¼ì œ ì¶”ì¶œ
        recommended_topics = news_analysis.get('recommended_topics', [])
        business_intent = news_analysis.get('business_intent', '')
        expected_challenges = news_analysis.get('expected_challenges', [])
        information_needs = news_analysis.get('information_needs', [])
        portone_solution = news_analysis.get('portone_solution', service_type or 'OPI')
        
        logger.info(f"ğŸ¯ {company_name} ìŠ¤ë§ˆíŠ¸ ë¸”ë¡œê·¸ ì¶”ì²œ ì‹œì‘")
        logger.info(f"   - ì˜ë„: {business_intent}")
        logger.info(f"   - ì–´ë ¤ì›€: {expected_challenges}")
        logger.info(f"   - ì¶”ì²œ ì£¼ì œ: {recommended_topics}")
        
        # ë¸”ë¡œê·¸ ê²€ìƒ‰ (ì„œë¹„ìŠ¤ ìœ í˜•ë³„ category í•„í„°ë§ - PostgreSQL)
        # âš ï¸ ì¤‘ìš”: OPI ë©”ì¼ì—” OPI ë¸”ë¡œê·¸ë§Œ, PSì—” PS ë¸”ë¡œê·¸ë§Œ, Prismì—” Prism ë¸”ë¡œê·¸ë§Œ!
        
        # ì„œë¹„ìŠ¤ ìœ í˜• ê²°ì • (ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ > ëª…ì‹œì  ì„œë¹„ìŠ¤ íƒ€ì…)
        effective_service = service_type.upper() if service_type else None
        
        # ë‰´ìŠ¤ ë¶„ì„ì—ì„œ ì¶”ì²œí•œ ì†”ë£¨ì…˜ì´ ìˆìœ¼ë©´ ì°¸ê³  (ë‹¨, ìš”ì²­ëœ ì„œë¹„ìŠ¤ íƒ€ì… ìš°ì„ )
        if not effective_service and portone_solution:
            effective_service = portone_solution.upper()
        
        query = db.session.query(BlogPost).order_by(BlogPost.created_at.desc())
        
        # ğŸ”¥ ì¹´í…Œê³ ë¦¬ë¡œ ì •í™•í•˜ê²Œ í•„í„°ë§ (OPI/PS/PRISM)
        if effective_service:
            # category í•„ë“œë¡œ í•„í„°ë§ (PostgreSQLì— ì €ì¥ëœ ë¶„ë¥˜ ì‚¬ìš©)
            query = query.filter(BlogPost.category == effective_service)
            logger.info(f"ğŸ” {effective_service} ë¸”ë¡œê·¸ë§Œ ê²€ìƒ‰ (category í•„í„°)")
        
        all_blogs = query.limit(50).all()
        
        # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë¸”ë¡œê·¸ê°€ ì—†ìœ¼ë©´ í´ë°± ì‹œë„í•˜ì§€ ì•ŠìŒ (í˜¼ìš© ë°©ì§€)
        if not all_blogs and effective_service:
            logger.warning(f"âš ï¸ {effective_service} ì¹´í…Œê³ ë¦¬ ë¸”ë¡œê·¸ ì—†ìŒ - ë¸”ë¡œê·¸ ì¶”ì²œ ìŠ¤í‚µ")
            return None
        
        if not all_blogs:
            logger.warning(f"ğŸ“ {company_name}: ë¸”ë¡œê·¸ ì—†ìŒ")
            return None
        
        # AIë¡œ ê°€ì¥ ì í•©í•œ ë¸”ë¡œê·¸ ì„ íƒ
        import google.generativeai as genai
        
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            # API ì—†ìœ¼ë©´ í‚¤ì›Œë“œ ë§¤ì¹­ í´ë°±
            basic_blog = get_best_blog_for_email_mention(company_info, research_data, service_type=effective_service)
            if basic_blog:
                return {
                    'primary_blog': basic_blog,
                    'supporting_blogs': [],
                    'recommendation_reason': basic_blog.get('match_reason', 'ì—…ì¢… ê´€ë ¨ ì‚¬ë¡€'),
                    'email_mention_text': _generate_email_mention_text(basic_blog, company_name)
                }
            return None
        
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash')
        
        # ë¸”ë¡œê·¸ ë¦¬ìŠ¤íŠ¸ êµ¬ì„± - ğŸ†• contentë„ í¬í•¨í•˜ì—¬ Geminiê°€ ì‹¤ì œ ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨
        blog_list = []
        for i, blog in enumerate(all_blogs[:15]):  # 15ê°œë¡œ ì¤„ì—¬ í† í° ì ˆì•½
            # ë¸”ë¡œê·¸ ë³¸ë¬¸ ë‚´ìš©ë„ í¬í•¨ (500ìê¹Œì§€)
            blog_content = (blog.content or '')[:500]
            blog_list.append({
                'index': i,
                'title': blog.title,
                'link': blog.link,
                'summary': (blog.summary or '')[:300],
                'content_preview': blog_content,  # ğŸ†• ì‹¤ì œ ë³¸ë¬¸ ë‚´ìš© ì¶”ê°€
                'target_audience': blog.target_audience or '',
                'pain_points': blog.pain_points_addressed or '',
                'case_company': blog.case_company or '',
                'case_industry': blog.case_industry or ''
            })
        
        # ì„œë¹„ìŠ¤ ìœ í˜• ì„¤ëª…
        service_descriptions = {
            'OPI': 'One Payment Infra - ê²°ì œ ì¸í”„ë¼ í†µí•©, PG ì—°ë™, ê°„í¸ê²°ì œ, í•´ì™¸ê²°ì œ',
            'PS': 'Partner Settlement - í”Œë«í¼ ì •ì‚°, íŒŒíŠ¸ë„ˆ ì •ì‚° ìë™í™”, ì „ìê¸ˆìœµë²• ëŒ€ì‘',
            'PRISM': 'Prism - ë©€í‹° ì˜¤í”ˆë§ˆì¼“ ì •ì‚° í†µí•©, ì±„ë„ í†µí•© ê´€ë¦¬, ë§¤ì¶œ ë¶„ì„'
        }
        service_desc = service_descriptions.get(effective_service, 'í¬íŠ¸ì› ì†”ë£¨ì…˜')
        
        # ğŸ†• research_dataì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
        research_summary = ""
        if research_data:
            # Perplexity ë¦¬ì„œì¹˜ ê²°ê³¼ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
            if research_data.get('summary'):
                research_summary += f"- ì¡°ì‚¬ ìš”ì•½: {research_data.get('summary', '')[:300]}\n"
            if research_data.get('key_findings'):
                findings = research_data.get('key_findings', [])
                if isinstance(findings, list):
                    research_summary += f"- ì£¼ìš” ë°œê²¬: {', '.join(findings[:3])}\n"
            if research_data.get('pain_points'):
                pains = research_data.get('pain_points', [])
                if isinstance(pains, list):
                    research_summary += f"- ì£¼ìš” ê³ ì¶©: {', '.join(pains[:3])}\n"
            if research_data.get('business_model'):
                research_summary += f"- ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸: {research_data.get('business_model', '')}\n"
            if research_data.get('competitors'):
                comps = research_data.get('competitors', [])
                if isinstance(comps, list):
                    research_summary += f"- ê²½ìŸì‚¬: {', '.join(comps[:3])}\n"
        
        prompt = f"""ë‹¤ìŒ íšŒì‚¬ì—ê²Œ ë³´ë‚¼ ì˜ì—… ì´ë©”ì¼ì— í¬í•¨í•  ê°€ì¥ ì í•©í•œ **{effective_service}** ë¸”ë¡œê·¸ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

ğŸ¯ **í•µì‹¬ ëª©í‘œ: ì´ íšŒì‚¬ì˜ "ì–´ë ¤ì›€ì„ í•´ê²°í•´ì¤„ ìˆ˜ ìˆë‹¤"ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ëŠ” ë¸”ë¡œê·¸ë¥¼ ì°¾ìœ¼ì„¸ìš”!**
- ì´ë©”ì¼ì—ì„œ "ë¹„ìŠ·í•œ ìƒí™©ì˜ ê³ ê°ì‚¬ê°€ ì´ë ‡ê²Œ í•´ê²°í–ˆë‹¤"ë¼ê³  ì–¸ê¸‰í•  ë¸”ë¡œê·¸
- íšŒì‚¬ê°€ "í•˜ë ¤ëŠ” ê²ƒ"ê³¼ "ì˜ˆìƒ ì–´ë ¤ì›€"ì— ì§ì ‘ì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ì‚¬ë¡€
- ì—…ì¢…ì´ ë‹¤ë¥´ë”ë¼ë„ "í•˜ë ¤ëŠ” ê²ƒ"ì´ë‚˜ "ì–´ë ¤ì›€"ì´ ìœ ì‚¬í•˜ë©´ ì¶”ì²œ ê°€ëŠ¥
  (ì˜ˆ: êµ¬ë… ì„œë¹„ìŠ¤ ë„ì… ì˜ˆì • íšŒì‚¬ì—ê²Œ ì—…ì¢…ì´ ë‹¬ë¼ë„ êµ¬ë… ê²°ì œ ë„ì… ì‚¬ë¡€ ë¸”ë¡œê·¸ âœ…)

**íšŒì‚¬ ì •ë³´:**
- íšŒì‚¬ëª…: {company_name}
- ì—…ì¢…: {industry}
- í•˜ë ¤ëŠ” ê²ƒ: {business_intent}
- ì˜ˆìƒ ì–´ë ¤ì›€: {', '.join(expected_challenges)}
- í•„ìš”í•œ ì •ë³´: {', '.join(information_needs)}

**Perplexity ë¦¬ì„œì¹˜ ê²°ê³¼ (AIê°€ ì¡°ì‚¬í•œ íšŒì‚¬ ì •ë³´):**
{research_summary if research_summary else 'ì¡°ì‚¬ ë°ì´í„° ì—†ìŒ'}

**{effective_service} ë¸”ë¡œê·¸ ëª©ë¡ ({len(blog_list)}ê°œ):**
ê° ë¸”ë¡œê·¸ì˜ **content_preview**ë¥¼ ê¼¼ê¼¼íˆ ì½ê³ , ì´ íšŒì‚¬ì˜ "í•˜ë ¤ëŠ” ê²ƒ"ê³¼ "ì–´ë ¤ì›€"ì„ í•´ê²°í•´ì¤„ ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.
{json.dumps(blog_list, ensure_ascii=False, indent=2)}

**ì„ íƒ ê¸°ì¤€ (ìš°ì„ ìˆœìœ„):**
1. ğŸ’¡ **ì–´ë ¤ì›€ í•´ê²° (ìµœìš°ì„ )**: íšŒì‚¬ì˜ "ì˜ˆìƒ ì–´ë ¤ì›€"ê³¼ ìœ ì‚¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•œ ì‚¬ë¡€ì¸ê°€?
2. ğŸ¯ **í•˜ë ¤ëŠ” ê²ƒ ì—°ê´€**: íšŒì‚¬ì˜ "í•˜ë ¤ëŠ” ê²ƒ"ê³¼ ì§ì ‘ ì—°ê²°ë˜ëŠ” ì‚¬ë¡€ì¸ê°€?
3. ğŸ­ **ì—…ì¢… ìœ ì‚¬ì„±**: ë™ì¼/ìœ ì‚¬ ì—…ì¢… ì‚¬ë¡€ë¼ë©´ ì‹ ë¢°ë„ ìƒìŠ¹
4. ğŸ“Š **êµ¬ì²´ì  ì„±ê³¼**: ìˆ˜ì¹˜ì™€ íš¨ê³¼ê°€ ëª…ì‹œë˜ì–´ ìˆëŠ”ê°€?

âš ï¸ **ì£¼ì˜:** ì—…ì¢…ì´ ë‹¤ë¥´ë”ë¼ë„ "í•˜ë ¤ëŠ” ê²ƒ"ì´ë‚˜ "ì–´ë ¤ì›€"ì´ ìœ ì‚¬í•˜ë©´ ì¶”ì²œí•˜ì„¸ìš”.
ë‹¨, ì™„ì „íˆ ê´€ë ¨ ì—†ëŠ” ë¸”ë¡œê·¸(ì˜ˆ: ê²°ì œ ë„ì… ì˜ˆì •ì¸ë° ì •ì‚° ì‚¬ë¡€ë§Œ ìˆëŠ” ê²½ìš°)ëŠ” ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”.

**JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:**
```json
{{
    "has_relevant_blog": true ë˜ëŠ” false (ì–´ë ¤ì›€ í•´ê²°ì— ë„ì›€ë˜ëŠ” ë¸”ë¡œê·¸ê°€ ìˆëŠ”ì§€),
    "relevance_score": 1-10 (íšŒì‚¬ ìƒí™©ê³¼ì˜ ì—°ê´€ì„± ì ìˆ˜, 6ì  ì´ìƒì´ì–´ì•¼ ì¶”ì²œ),
    "primary_index": ê°€ì¥ ì¶”ì²œí•˜ëŠ” ë¸”ë¡œê·¸ ì¸ë±ìŠ¤ (ìˆ«ì, ì í•©í•œ ë¸”ë¡œê·¸ ì—†ìœ¼ë©´ -1),
    "supporting_indices": [2ìˆœìœ„, 3ìˆœìœ„ ë¸”ë¡œê·¸ ì¸ë±ìŠ¤],
    "solution_match": "ì´ ë¸”ë¡œê·¸ê°€ í•´ê²°í•´ì£¼ëŠ” ë¬¸ì œ (ì˜ˆ: PG í†µí•© ë³µì¡ì„±, ì •ì‚° ìë™í™” ë“±)",
    "recommendation_reason": "ì´ ë¸”ë¡œê·¸ë¥¼ ì¶”ì²œí•˜ëŠ” ì´ìœ  (íšŒì‚¬ì˜ ì–´ë ¤ì›€ê³¼ ì–´ë–»ê²Œ ì—°ê²°ë˜ëŠ”ì§€)",
    "email_hook": "ì´ë©”ì¼ì—ì„œ ì´ ë¸”ë¡œê·¸ë¥¼ ì–¸ê¸‰í•  ë•Œ ì‚¬ìš©í•  í›„í‚¹ ë¬¸êµ¬ (ì˜ˆ: 'XX ê³ ê°ì‚¬ë„ ë¹„ìŠ·í•œ ê³ ë¯¼ì„ í•˜ì…¨ëŠ”ë°...')",
    "rejection_reason": "ì í•©í•œ ë¸”ë¡œê·¸ê°€ ì—†ëŠ” ê²½ìš° ê·¸ ì´ìœ  (has_relevant_blogê°€ falseì¼ ë•Œë§Œ)"
}}
```

âš ï¸ ì í•©í•œ ë¸”ë¡œê·¸ê°€ ì—†ìœ¼ë©´ has_relevant_blog: false, primary_index: -1ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
ì˜ëª»ëœ ë¸”ë¡œê·¸ë¥¼ ì¶”ì²œí•˜ëŠ” ê²ƒë³´ë‹¤ ì¶”ì²œí•˜ì§€ ì•ŠëŠ” ê²ƒì´ ë‚«ìŠµë‹ˆë‹¤.
"""
        
        response = model.generate_content(prompt)
        
        if response and response.text:
            result_text = response.text.strip()
            
            if '```json' in result_text:
                result_text = result_text.split('```json')[1].split('```')[0].strip()
            elif '```' in result_text:
                result_text = result_text.split('```')[1].split('```')[0].strip()
            
            try:
                selection = json.loads(result_text)
                
                # ğŸ†• ì—…ì¢… ì—°ê´€ì„± ì²´í¬ - ê´€ë ¨ ì—†ìœ¼ë©´ ì¶”ì²œí•˜ì§€ ì•ŠìŒ
                has_relevant_blog = selection.get('has_relevant_blog', True)
                relevance_score = selection.get('relevance_score', 0)
                rejection_reason = selection.get('rejection_reason', '')
                
                # ê´€ë ¨ì„± ì ìˆ˜ê°€ 6ì  ë¯¸ë§Œì´ê±°ë‚˜ ê´€ë ¨ ë¸”ë¡œê·¸ê°€ ì—†ìœ¼ë©´ ì¶”ì²œí•˜ì§€ ì•ŠìŒ
                if not has_relevant_blog or relevance_score < 6:
                    logger.info(f"âš ï¸ {company_name}: íšŒì‚¬ ìƒí™©ê³¼ ì—°ê´€ì„± ë‚®ìŒ - ë¸”ë¡œê·¸ ì¶”ì²œ ìŠ¤í‚µ (ì ìˆ˜: {relevance_score}/10, ì´ìœ : {rejection_reason})")
                    return None
                
                primary_idx = selection.get('primary_index', -1)
                supporting_indices = selection.get('supporting_indices', [])
                recommendation_reason = selection.get('recommendation_reason', '')
                email_hook = selection.get('email_hook', '')
                solution_match = selection.get('solution_match', '')  # ğŸ†• í•´ê²°í•˜ëŠ” ë¬¸ì œ
                
                # primary_indexê°€ -1ì´ë©´ ì í•©í•œ ë¸”ë¡œê·¸ ì—†ìŒ
                if primary_idx < 0:
                    logger.info(f"âš ï¸ {company_name}: Geminiê°€ ì í•©í•œ ë¸”ë¡œê·¸ ì—†ë‹¤ê³  íŒë‹¨ - ì¶”ì²œ ìŠ¤í‚µ")
                    return None
                
                # ë©”ì¸ ë¸”ë¡œê·¸
                if 0 <= primary_idx < len(all_blogs):
                    primary_blog_obj = all_blogs[primary_idx]
                    primary_blog = {
                        'title': primary_blog_obj.title,
                        'link': primary_blog_obj.link,
                        'summary': primary_blog_obj.summary or '',
                        'match_reason': recommendation_reason,
                        'case_company': primary_blog_obj.case_company,
                        'industry_matched': True
                    }
                    
                    # ì„œí¬íŒ… ë¸”ë¡œê·¸
                    supporting_blogs = []
                    for idx in supporting_indices[:2]:
                        if 0 <= idx < len(all_blogs):
                            blog = all_blogs[idx]
                            supporting_blogs.append({
                                'title': blog.title,
                                'link': blog.link,
                                'summary': blog.summary or ''
                            })
                    
                    # ì´ë©”ì¼ ì–¸ê¸‰ ë¬¸êµ¬ ìƒì„±
                    email_mention = _generate_smart_email_mention(
                        primary_blog, 
                        company_name, 
                        business_intent, 
                        email_hook
                    )
                    
                    logger.info(f"âœ… {company_name}: ìŠ¤ë§ˆíŠ¸ ë¸”ë¡œê·¸ ì¶”ì²œ - {primary_blog['title'][:40]}... (ì—°ê´€ì„±: {relevance_score}/10, í•´ê²°ë¬¸ì œ: {solution_match})")
                    
                    return {
                        'primary_blog': primary_blog,
                        'supporting_blogs': supporting_blogs,
                        'recommendation_reason': recommendation_reason,
                        'email_mention_text': email_mention,
                        'news_analysis': news_analysis
                    }
                    
            except json.JSONDecodeError as e:
                logger.warning(f"ë¸”ë¡œê·¸ ì„ íƒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ í´ë°±
        basic_blog = get_best_blog_for_email_mention(company_info, research_data, service_type=effective_service)
        if basic_blog:
            return {
                'primary_blog': basic_blog,
                'supporting_blogs': [],
                'recommendation_reason': basic_blog.get('match_reason', 'ì—…ì¢… ê´€ë ¨ ì‚¬ë¡€'),
                'email_mention_text': _generate_email_mention_text(basic_blog, company_name)
            }
        
        return None
        
    except Exception as e:
        logger.error(f"ìŠ¤ë§ˆíŠ¸ ë¸”ë¡œê·¸ ì¶”ì²œ ì˜¤ë¥˜: {str(e)}")
        return None


def _generate_email_mention_text(blog_info, company_name):
    """ê¸°ë³¸ ì´ë©”ì¼ ì–¸ê¸‰ ë¬¸êµ¬ ìƒì„±"""
    if not blog_info:
        return ""
    
    title = blog_info.get('title', '')
    link = blog_info.get('link', '')
    reason = blog_info.get('match_reason', '')
    case_company = blog_info.get('case_company', '')
    
    if case_company:
        mention = f"ë¹„ìŠ·í•œ ê³ ë¯¼ì„ í•˜ì…¨ë˜ {case_company} ì‚¬ë¡€ê°€ ìˆëŠ”ë°ìš”, ì•„ë˜ ê¸€ì—ì„œ ìì„¸íˆ í™•ì¸í•´ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    elif reason:
        mention = f"{reason} ê´€ë ¨í•´ì„œ ë„ì›€ì´ ë  ë§Œí•œ ì‚¬ë¡€ê°€ ìˆì–´ìš”."
    else:
        mention = "ë¹„ìŠ·í•œ ìƒí™©ì˜ ê³ ê°ì‚¬ ì‚¬ë¡€ê°€ ìˆì–´ ê³µìœ ë“œë¦½ë‹ˆë‹¤."
    
    return f"""{mention}

ğŸ‘‰ {title}
{link}"""


def _generate_smart_email_mention(blog_info, company_name, business_intent, email_hook=''):
    """
    ğŸ†• ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜ì˜í•œ ìŠ¤ë§ˆíŠ¸ ì´ë©”ì¼ ì–¸ê¸‰ ë¬¸êµ¬ ìƒì„±
    """
    if not blog_info:
        return ""
    
    title = blog_info.get('title', '')
    link = blog_info.get('link', '')
    case_company = blog_info.get('case_company', '')
    
    # AIê°€ ìƒì„±í•œ í›„í‚¹ ë¬¸êµ¬ ì‚¬ìš©
    if email_hook:
        mention = email_hook
    elif business_intent:
        # ì˜ë„ ê¸°ë°˜ ë¬¸êµ¬ ìƒì„±
        intent_phrases = {
            'í•´ì™¸': f"{company_name}ë‹˜ì˜ í•´ì™¸ ì§„ì¶œ ê³„íšê³¼ ê´€ë ¨í•´ì„œ",
            'ê¸€ë¡œë²Œ': f"ê¸€ë¡œë²Œ ì‹œì¥ ì§„ì¶œ ì‹œ ê²°ì œ ì‹œìŠ¤í…œ êµ¬ì¶•ì— ëŒ€í•´",
            'êµ¬ë…': f"êµ¬ë… ì„œë¹„ìŠ¤ ê²°ì œ ì‹œìŠ¤í…œ êµ¬ì¶•ì— ìˆì–´",
            'ì •ì‚°': f"ì •ì‚° ì—…ë¬´ íš¨ìœ¨í™”ì™€ ê´€ë ¨í•´ì„œ",
            'í”Œë«í¼': f"í”Œë«í¼ ë¹„ì¦ˆë‹ˆìŠ¤ì˜ íŒŒíŠ¸ë„ˆ ì •ì‚°ì— ëŒ€í•´",
            'ì´ì»¤ë¨¸ìŠ¤': f"ì´ì»¤ë¨¸ìŠ¤ ê²°ì œ ì‹œìŠ¤í…œ ê°œì„ ê³¼ ê´€ë ¨í•´ì„œ"
        }
        
        for keyword, phrase in intent_phrases.items():
            if keyword in business_intent.lower():
                mention = f"{phrase}, ë¹„ìŠ·í•œ ê³ ë¯¼ì„ í•˜ì…¨ë˜ ê³ ê°ì‚¬ ì‚¬ë¡€ê°€ ìˆì–´ìš”."
                break
        else:
            mention = f"{company_name}ë‹˜ê³¼ ë¹„ìŠ·í•œ ìƒí™©ì˜ ê³ ê°ì‚¬ ì‚¬ë¡€ê°€ ìˆì–´ ê³µìœ ë“œë¦½ë‹ˆë‹¤."
    else:
        mention = "ë¹„ìŠ·í•œ ê³ ë¯¼ì„ í•˜ì…¨ë˜ ê³ ê°ì‚¬ì˜ ì„±ê³µ ì‚¬ë¡€ê°€ ìˆì–´ìš”."
    
    # ê³ ê°ì‚¬ëª…ì´ ìˆìœ¼ë©´ ë” êµ¬ì²´ì ìœ¼ë¡œ
    if case_company and case_company not in mention:
        mention = mention.replace("ê³ ê°ì‚¬", case_company)
    
    return f"""{mention}

ğŸ‘‰ {title}
{link}"""
