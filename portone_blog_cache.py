"""
í¬íŠ¸ì› ë¸”ë¡œê·¸ ì½˜í…ì¸  ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œ
"""

import sqlite3
from datetime import datetime
import logging
import json

logger = logging.getLogger(__name__)

DB_FILE = 'portone_blog.db'

def init_db():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ë° í…Œì´ë¸” ìƒì„±"""
    try:
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        
        # ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS blog_posts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                link TEXT UNIQUE,
                summary TEXT,
                content TEXT,
                category TEXT,
                keywords TEXT,
                industry_tags TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ìºì‹œ ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS cache_metadata (
                id INTEGER PRIMARY KEY CHECK (id = 1),
                last_updated TIMESTAMP,
                posts_count INTEGER
            )
        ''')
        
        # ì¸ë±ìŠ¤ ìƒì„±
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_created_at 
            ON blog_posts(created_at DESC)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_category 
            ON blog_posts(category)
        ''')
        
        conn.commit()
        conn.close()
        
        logger.info("âœ… ë¸”ë¡œê·¸ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        return True
    except Exception as e:
        logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
        return False

def save_blog_cache(blog_posts, replace_all=True):
    """ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    try:
        init_db()
        
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        
        # replace_allì´ Trueë©´ ê¸°ì¡´ í¬ìŠ¤íŠ¸ ì‚­ì œ
        if replace_all:
            cursor.execute('DELETE FROM blog_posts')
        
        # ìƒˆ í¬ìŠ¤íŠ¸ ì‚½ì… (ì¤‘ë³µì€ ë¬´ì‹œ)
        inserted_count = 0
        for post in blog_posts:
            try:
                cursor.execute('''
                    INSERT INTO blog_posts (title, link, summary, content, category, keywords, industry_tags, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    post.get('title', ''),
                    post.get('link', ''),
                    post.get('summary', ''),
                    post.get('content', ''),
                    post.get('category', ''),
                    post.get('keywords', ''),
                    post.get('industry_tags', ''),
                    datetime.now(),
                    datetime.now()
                ))
                inserted_count += 1
            except sqlite3.IntegrityError:
                # ì¤‘ë³µ ë§í¬ëŠ” ì—…ë°ì´íŠ¸
                cursor.execute('''
                    UPDATE blog_posts 
                    SET title=?, summary=?, content=?, category=?, keywords=?, industry_tags=?, updated_at=?
                    WHERE link=?
                ''', (
                    post.get('title', ''),
                    post.get('summary', ''),
                    post.get('content', ''),
                    post.get('category', ''),
                    post.get('keywords', ''),
                    post.get('industry_tags', ''),
                    datetime.now(),
                    post.get('link', '')
                ))
                inserted_count += 1
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        cursor.execute('SELECT COUNT(*) FROM blog_posts')
        total_count = cursor.fetchone()[0]
        
        cursor.execute('''
            INSERT OR REPLACE INTO cache_metadata (id, last_updated, posts_count)
            VALUES (1, ?, ?)
        ''', (datetime.now(), total_count))
        
        conn.commit()
        conn.close()
        
        logger.info(f"âœ… ë¸”ë¡œê·¸ DB ì €ì¥ ì™„ë£Œ: {inserted_count}ê°œ ê¸€ ì²˜ë¦¬, ì´ {total_count}ê°œ")
        return True
    except Exception as e:
        logger.error(f"ë¸”ë¡œê·¸ DB ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        return False

def load_blog_cache():
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ë¡œë“œ"""
    try:
        init_db()
        
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT title, link, summary, content, category, keywords, industry_tags, created_at
            FROM blog_posts
            ORDER BY created_at DESC
        ''')
        
        rows = cursor.fetchall()
        conn.close()
        
        if not rows:
            logger.info("ğŸ“ ë¸”ë¡œê·¸ DBì— ë°ì´í„° ì—†ìŒ")
            return None
        
        posts = []
        for row in rows:
            posts.append({
                'title': row[0],
                'link': row[1],
                'summary': row[2],
                'content': row[3],
                'category': row[4],
                'keywords': row[5],
                'industry_tags': row[6],
                'created_at': row[7]
            })
        
        logger.info(f"ğŸ“š ë¸”ë¡œê·¸ DB ë¡œë“œ ì™„ë£Œ: {len(posts)}ê°œ ê¸€")
        return posts
    except Exception as e:
        logger.error(f"ë¸”ë¡œê·¸ DB ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return None

def get_blog_cache_age():
    """ë°ì´í„°ë² ì´ìŠ¤ì˜ ì—…ë°ì´íŠ¸ ì‹œê°„ í™•ì¸"""
    try:
        init_db()
        
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        
        cursor.execute('SELECT last_updated FROM cache_metadata WHERE id = 1')
        row = cursor.fetchone()
        conn.close()
        
        if not row or not row[0]:
            return None
        
        # SQLite datetimeì„ Python datetimeìœ¼ë¡œ ë³€í™˜
        if isinstance(row[0], str):
            updated_time = datetime.fromisoformat(row[0].replace(' ', 'T'))
        else:
            updated_time = row[0]
        
        age_hours = (datetime.now() - updated_time).total_seconds() / 3600
        return age_hours
    except Exception as e:
        logger.error(f"ìºì‹œ ì‹œê°„ í™•ì¸ ì˜¤ë¥˜: {str(e)}")
        return None

def format_blog_content_for_email(blog_posts):
    """ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì´ë©”ì¼ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
    if not blog_posts:
        return ""
    
    content = "\n\n**ğŸ“° í¬íŠ¸ì› ìµœì‹  ë¸”ë¡œê·¸ ì½˜í…ì¸  (ì°¸ê³ ìš©):**\n"
    content += "ì•„ë˜ ìµœì‹  ì½˜í…ì¸ ë¥¼ ì°¸ê³ í•˜ì—¬ ë©”ì¼ ì‘ì„± ì‹œ ìì—°ìŠ¤ëŸ½ê²Œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
    
    for i, post in enumerate(blog_posts[:5], 1):  # ìµœëŒ€ 5ê°œ
        content += f"{i}. **{post['title']}**\n"
        if post.get('summary'):
            content += f"   {post['summary'][:150]}...\n"
        if post.get('link'):
            content += f"   ë§í¬: {post['link']}\n"
        content += "\n"
    
    content += "ğŸ’¡ ìœ„ ì½˜í…ì¸ ë¥¼ í™œìš©í•˜ì—¬ ìµœì‹  íŠ¸ë Œë“œë‚˜ í¬íŠ¸ì›ì˜ ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
    
    return content

def extract_keywords_from_post(post):
    """ë¸”ë¡œê·¸ ê¸€ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (Gemini í™œìš©)"""
    try:
        content = post.get('content', '')
        title = post.get('title', '')
        
        if not content or len(content) < 50:
            return '', ''
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë‚˜ì¤‘ì— Geminië¡œ ê°•í™” ê°€ëŠ¥)
        # í˜„ì¬ëŠ” ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ìˆœ íƒœê·¸ ìƒì„±
        category = post.get('category', '')
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ
        keywords = []
        industry_tags = []
        
        # ì œëª©ê³¼ ë‚´ìš©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì°¾ê¸°
        text_lower = (title + ' ' + content[:500]).lower()
        
        # ì—…ì¢… ê´€ë ¨ í‚¤ì›Œë“œ
        if 'ê²Œì„' in text_lower or 'game' in text_lower:
            industry_tags.append('ê²Œì„')
        if 'ì´ì»¤ë¨¸ìŠ¤' in text_lower or 'eì»¤ë¨¸ìŠ¤' in text_lower or 'ì‡¼í•‘ëª°' in text_lower or 'commerce' in text_lower:
            industry_tags.append('ì´ì»¤ë¨¸ìŠ¤')
        if 'ì—¬í–‰' in text_lower or 'travel' in text_lower or 'í•­ê³µ' in text_lower:
            industry_tags.append('ì—¬í–‰')
        if 'êµìœ¡' in text_lower or 'education' in text_lower or 'ì—ë“€í…Œí¬' in text_lower:
            industry_tags.append('êµìœ¡')
        if 'ê¸ˆìœµ' in text_lower or 'fintech' in text_lower or 'í•€í…Œí¬' in text_lower:
            industry_tags.append('ê¸ˆìœµ')
        if 'ë¯¸ë””ì–´' in text_lower or 'media' in text_lower or 'ì½˜í…ì¸ ' in text_lower:
            industry_tags.append('ë¯¸ë””ì–´')
        if 'saas' in text_lower or 'êµ¬ë…' in text_lower:
            industry_tags.append('SaaS')
        if 'ë¬¼ë¥˜' in text_lower or 'logistics' in text_lower or 'ë°°ì†¡' in text_lower:
            industry_tags.append('ë¬¼ë¥˜')
        
        # ê¸°ëŠ¥ ê´€ë ¨ í‚¤ì›Œë“œ
        if 'ê²°ì œ' in text_lower or 'payment' in text_lower:
            keywords.append('ê²°ì œ')
        if 'ì •ì‚°' in text_lower or 'ë§¤ì¶œ' in text_lower or 'reconciliation' in text_lower:
            keywords.append('ë§¤ì¶œê´€ë¦¬')
        if 'ìë™í™”' in text_lower or 'automation' in text_lower:
            keywords.append('ìë™í™”')
        if 'pg' in text_lower or 'ê°„í¸ê²°ì œ' in text_lower:
            keywords.append('PG')
        if 'í•´ì™¸' in text_lower or 'global' in text_lower or 'ê¸€ë¡œë²Œ' in text_lower:
            keywords.append('ê¸€ë¡œë²Œ')
        if 'ì •ê¸°ê²°ì œ' in text_lower or 'subscription' in text_lower:
            keywords.append('ì •ê¸°ê²°ì œ')
        
        return ','.join(keywords), ','.join(industry_tags)
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
        return '', ''

def get_relevant_blog_posts_by_industry(company_info, max_posts=3, service_type=None):
    """
    íšŒì‚¬ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë¸”ë¡œê·¸ ê¸€ ì¡°íšŒ
    
    Args:
        company_info: íšŒì‚¬ ì •ë³´ ë”•ì…”ë„ˆë¦¬ (industry, category, description ë“±)
        max_posts: ìµœëŒ€ ë°˜í™˜ ê¸€ ìˆ˜
        service_type: ì„œë¹„ìŠ¤ íƒ€ì… ('OPI' ë˜ëŠ” 'Recon', Noneì´ë©´ ëª¨ë‘ ì¡°íšŒ)
    
    Returns:
        list: ê´€ë ¨ ë¸”ë¡œê·¸ ê¸€ ë¦¬ìŠ¤íŠ¸
    """
    try:
        init_db()
        
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        
        # íšŒì‚¬ ì—…ì¢…/ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        industry = company_info.get('industry', '')
        category = company_info.get('category', '')
        description = company_info.get('description', '')
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ êµ¬ì„±
        search_terms = []
        if industry:
            search_terms.append(industry)
        if category:
            search_terms.append(category)
        
        # ì„¤ëª…ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        if description:
            desc_lower = description.lower()
            if 'ê²Œì„' in desc_lower or 'game' in desc_lower:
                search_terms.append('ê²Œì„')
            if 'ì´ì»¤ë¨¸ìŠ¤' in desc_lower or 'ì‡¼í•‘ëª°' in desc_lower:
                search_terms.append('ì´ì»¤ë¨¸ìŠ¤')
            if 'ì—¬í–‰' in desc_lower or 'travel' in desc_lower:
                search_terms.append('ì—¬í–‰')
            if 'êµìœ¡' in desc_lower or 'education' in desc_lower:
                search_terms.append('êµìœ¡')
            if 'ê¸ˆìœµ' in desc_lower or 'fintech' in desc_lower:
                search_terms.append('ê¸ˆìœµ')
        
        # ì„œë¹„ìŠ¤ íƒ€ì… ì¡°ê±´ ì¶”ê°€
        service_condition = ''
        params = []
        
        if service_type:
            service_condition = 'category = ?'
            params.append(service_type)
        
        if not search_terms:
            # ê²€ìƒ‰ì–´ê°€ ì—†ìœ¼ë©´ ìµœì‹  ê¸€ ë°˜í™˜ (ì„œë¹„ìŠ¤ íƒ€ì… í•„í„°ë§)
            if service_condition:
                query = f'''
                    SELECT title, link, summary, content, category, keywords, industry_tags
                    FROM blog_posts
                    WHERE {service_condition}
                    ORDER BY created_at DESC
                    LIMIT ?
                '''
                params.append(max_posts)
            else:
                query = '''
                    SELECT title, link, summary, content, category, keywords, industry_tags
                    FROM blog_posts
                    ORDER BY created_at DESC
                    LIMIT ?
                '''
                params = [max_posts]
            
            cursor.execute(query, params)
        else:
            # ì—…ì¢… íƒœê·¸ ë˜ëŠ” í‚¤ì›Œë“œ ë§¤ì¹­ + ì„œë¹„ìŠ¤ íƒ€ì… í•„í„°ë§
            search_pattern = '%' + '%'.join(search_terms) + '%'
            
            if service_condition:
                query = f'''
                    SELECT title, link, summary, content, category, keywords, industry_tags
                    FROM blog_posts
                    WHERE {service_condition}
                      AND (industry_tags LIKE ? OR keywords LIKE ? OR title LIKE ? OR content LIKE ?)
                    ORDER BY created_at DESC
                    LIMIT ?
                '''
                params.extend([search_pattern, search_pattern, search_pattern, search_pattern, max_posts])
            else:
                query = '''
                    SELECT title, link, summary, content, category, keywords, industry_tags
                    FROM blog_posts
                    WHERE industry_tags LIKE ? OR keywords LIKE ? OR title LIKE ? OR content LIKE ?
                    ORDER BY created_at DESC
                    LIMIT ?
                '''
                params = [search_pattern, search_pattern, search_pattern, search_pattern, max_posts]
            
            cursor.execute(query, params)
        
        rows = cursor.fetchall()
        conn.close()
        
        service_label = f"[{service_type}] " if service_type else ""
        
        if not rows:
            if search_terms:
                logger.info(f"ğŸ” {service_label}'{', '.join(search_terms)}' ê´€ë ¨ ë¸”ë¡œê·¸ ê¸€ ì—†ìŒ")
            else:
                logger.info(f"ğŸ” {service_label}ë¸”ë¡œê·¸ ê¸€ ì—†ìŒ")
            return []
        
        posts = []
        for row in rows:
            posts.append({
                'title': row[0],
                'link': row[1],
                'summary': row[2],
                'content': row[3],
                'category': row[4],
                'keywords': row[5],
                'industry_tags': row[6]
            })
        
        if search_terms:
            logger.info(f"âœ… {service_label}'{', '.join(search_terms)}' ê´€ë ¨ ë¸”ë¡œê·¸ ê¸€ {len(posts)}ê°œ ì¡°íšŒ")
        else:
            logger.info(f"âœ… {service_label}ë¸”ë¡œê·¸ ê¸€ {len(posts)}ê°œ ì¡°íšŒ")
        return posts
    except Exception as e:
        logger.error(f"ì—…ì¢…ë³„ ë¸”ë¡œê·¸ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return []

def format_relevant_blog_for_email(blog_posts, company_name='', service_type=''):
    """
    ì—…ì¢…ë³„ ê´€ë ¨ ë¸”ë¡œê·¸ ê¸€ì„ RAG ë°©ì‹ìœ¼ë¡œ í¬ë§·íŒ… (ì§ì ‘ ì–¸ê¸‰ ì œê±°)
    
    Args:
        blog_posts: ë¸”ë¡œê·¸ ê¸€ ë¦¬ìŠ¤íŠ¸
        company_name: íšŒì‚¬ëª… (ê°œì¸í™”ìš©)
        service_type: ì„œë¹„ìŠ¤ íƒ€ì… ('OPI' ë˜ëŠ” 'Recon')
    
    Returns:
        str: í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸ (RAGìš© ì»¨í…ìŠ¤íŠ¸)
    """
    if not blog_posts:
        return ''
    
    service_label = service_type if service_type else 'í¬íŠ¸ì›'
    
    content = f"\n\n**ğŸ“š {service_label} ê´€ë ¨ ì°¸ê³  ì •ë³´ (RAG - ë¸”ë¡œê·¸ ì§ì ‘ ì–¸ê¸‰ ê¸ˆì§€!):**\n\n"
    content += "âš ï¸ **ì¤‘ìš” ì§€ì¹¨**: ì•„ë˜ ì •ë³´ëŠ” ì´ë©”ì¼ ë³¸ë¬¸ì˜ ì„¤ë“ë ¥ì„ ë†’ì´ê¸° ìœ„í•œ ì°¸ê³  ìë£Œì…ë‹ˆë‹¤.\n"
    content += "- ë¸”ë¡œê·¸ ê¸€ì„ ì§ì ‘ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš” (\"ìµœê·¼ í¬íŠ¸ì› ë¸”ë¡œê·¸ì—ì„œ...\" âŒ)\n"
    content += "- ì •ë³´ë§Œ ìì—°ìŠ¤ëŸ½ê²Œ í™œìš©í•˜ì—¬ ê·¼ê±° ìˆëŠ” ì£¼ì¥ì„ í¼ì¹˜ì„¸ìš”\n"
    content += "- ìˆ˜ì¹˜, íŠ¸ë Œë“œ, ì‚¬ë¡€ ë“±ì„ ìì‹ ì˜ ë§ë¡œ ë…¹ì—¬ì„œ ì‚¬ìš©í•˜ì„¸ìš”\n\n"
    
    content += "---\n\n"
    
    for i, post in enumerate(blog_posts[:3], 1):
        content += f"**ì°¸ê³ ìë£Œ {i}:**\n"
        
        # ì œëª©ì€ í‘œì‹œí•˜ë˜, ì´ë©”ì¼ì— ì§ì ‘ ì“°ì§€ ë§ë¼ê³  ëª…ì‹œ
        content += f"ì£¼ì œ: {post['title']}\n\n"
        
        # í•µì‹¬ ë‚´ìš© ì¶”ì¶œ (ìš”ì•½ + ë³¸ë¬¸ ì¼ë¶€)
        summary = post.get('summary', '')
        full_content = post.get('content', '')
        
        if summary:
            content += f"í•µì‹¬ ë‚´ìš©:\n{summary}\n\n"
        
        # ë³¸ë¬¸ì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ (ìˆ˜ì¹˜, í†µê³„, ì‚¬ë¡€ ë“±)
        if full_content and len(full_content) > len(summary):
            additional = full_content[len(summary):min(len(summary)+300, len(full_content))]
            content += f"ì¶”ê°€ ì •ë³´:\n{additional}...\n\n"
        
        content += "---\n\n"
    
    content += f"ğŸ’¡ **í™œìš© ë°©ë²•**: ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ {company_name}ì—ê²Œ {service_label} ì„œë¹„ìŠ¤ê°€ "
    content += "ì–´ë–»ê²Œ ë„ì›€ì´ ë˜ëŠ”ì§€ êµ¬ì²´ì ì´ê³  ì„¤ë“ë ¥ ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”.\n"
    content += "- ì—…ê³„ íŠ¸ë Œë“œë‚˜ Pain Pointë¥¼ ì–¸ê¸‰í•  ë•Œ ìœ„ ì •ë³´ í™œìš©\n"
    content += "- \"ë§ì€ ê¸°ì—…ë“¤ì´ X ë¬¸ì œë¥¼ ê²ªê³  ìˆìŠµë‹ˆë‹¤\" ê°™ì€ í‘œí˜„ì— ê·¼ê±° ì œì‹œ\n"
    content += "- ìˆ˜ì¹˜ë‚˜ ì‚¬ë¡€ê°€ ìˆë‹¤ë©´ \"ì—…ê³„ í‰ê· \", \"ë‹¤ë¥¸ ê¸°ì—… ì‚¬ë¡€\" ë“±ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì¸ìš©\n"
    
    return content

def get_service_knowledge(service_type=''):
    """
    ì„œë¹„ìŠ¤ ì†Œê°œì„œì™€ ë¸”ë¡œê·¸ ì „ì²´ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ RAG ì§€ì‹ë² ì´ìŠ¤ ìƒì„±
    
    Args:
        service_type: 'OPI', 'Recon', ë˜ëŠ” 'Prism'
    
    Returns:
        str: í†µí•©ëœ ì§€ì‹ë² ì´ìŠ¤ í…ìŠ¤íŠ¸
    """
    knowledge = ""
    
    # ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í•‘ì€ app.pyì˜ generate_email_with_geminiì—ì„œ ì²˜ë¦¬ë¨
    # ì—¬ê¸°ì„œëŠ” ì´ë¯¸ ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„°ë¥¼ ë¡œë“œë§Œ í•¨
    
    # 1. ì„œë¹„ìŠ¤ ì†Œê°œì„œ ë¡œë“œ
    if service_type == 'OPI':
        try:
            with open('opi_service_info.txt', 'r', encoding='utf-8') as f:
                service_doc = f.read()
            knowledge += f"\n\n**ğŸ“– One Payment Infra (OPI) ì„œë¹„ìŠ¤ ì†Œê°œ:**\n\n"
            knowledge += f"{service_doc[:3000]}...\n\n"  # ì²˜ìŒ 3000ì
            logger.info("âœ… OPI ì„œë¹„ìŠ¤ ì†Œê°œì„œ ë¡œë“œ ì™„ë£Œ")
        except:
            logger.warning("âš ï¸ OPI ì„œë¹„ìŠ¤ ì†Œê°œì„œ íŒŒì¼ ì—†ìŒ")
    
    elif service_type == 'Recon':
        try:
            with open('recon_service_info.txt', 'r', encoding='utf-8') as f:
                service_doc = f.read()
            knowledge += f"\n\n**ğŸ“– ì¬ë¬´ìë™í™” ì†”ë£¨ì…˜ (Recon) ì„œë¹„ìŠ¤ ì†Œê°œ:**\n\n"
            knowledge += f"{service_doc[:2000]}...\n\n"  # ì²˜ìŒ 2000ì
            logger.info("âœ… Recon ì„œë¹„ìŠ¤ ì†Œê°œì„œ ë¡œë“œ ì™„ë£Œ")
        except:
            logger.warning("âš ï¸ Recon ì„œë¹„ìŠ¤ ì†Œê°œì„œ íŒŒì¼ ì—†ìŒ")
    
    elif service_type == 'Prism':
        try:
            with open('prism_service_info.txt', 'r', encoding='utf-8') as f:
                service_doc = f.read()
            knowledge += f"\n\n**ğŸ“– ë©€í‹° ì˜¤í”ˆë§ˆì¼“ ì •ì‚° í†µí•© ì†”ë£¨ì…˜ (Prism) ì„œë¹„ìŠ¤ ì†Œê°œ:**\n\n"
            knowledge += f"{service_doc[:3000]}...\n\n"  # ì²˜ìŒ 3000ì
            logger.info("âœ… Prism ì„œë¹„ìŠ¤ ì†Œê°œì„œ ë¡œë“œ ì™„ë£Œ")
        except:
            logger.warning("âš ï¸ Prism ì„œë¹„ìŠ¤ ì†Œê°œì„œ íŒŒì¼ ì—†ìŒ")
    
    elif service_type == 'PS':
        try:
            with open('ps_service_info.txt', 'r', encoding='utf-8') as f:
                service_doc = f.read()
            knowledge += f"\n\n**ğŸ“– í”Œë«í¼ ì •ì‚° ìë™í™” (íŒŒíŠ¸ë„ˆ ì •ì‚°+ì„¸ê¸ˆê³„ì‚°ì„œ+ì§€ê¸‰ëŒ€í–‰) ì„œë¹„ìŠ¤ ì†Œê°œ:**\n\n"
            knowledge += f"{service_doc[:3500]}...\n\n"  # ì²˜ìŒ 3500ì
            logger.info("âœ… í”Œë«í¼ ì •ì‚°(PS) ì„œë¹„ìŠ¤ ì†Œê°œì„œ ë¡œë“œ ì™„ë£Œ")
        except:
            logger.warning("âš ï¸ í”Œë«í¼ ì •ì‚°(PS) ì„œë¹„ìŠ¤ ì†Œê°œì„œ íŒŒì¼ ì—†ìŒ")
    
    # 2. ë¸”ë¡œê·¸ ì „ì²´ ìš”ì•½ (í•´ë‹¹ ì¹´í…Œê³ ë¦¬)
    try:
        init_db()
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT title, summary, keywords
            FROM blog_posts
            WHERE category = ?
            ORDER BY created_at DESC
        ''', (service_type,))
        
        rows = cursor.fetchall()
        conn.close()
        
        if rows:
            knowledge += f"\n\n**ğŸ“š {service_type} ê´€ë ¨ ë¸”ë¡œê·¸ ì¸ì‚¬ì´íŠ¸ ({len(rows)}ê°œ ê¸€):**\n\n"
            knowledge += f"ë‹¤ìŒì€ í¬íŠ¸ì› ê³µì‹ ë¸”ë¡œê·¸ì—ì„œ {service_type} ê´€ë ¨ {len(rows)}ê°œ ê¸€ì˜ í•µì‹¬ ë‚´ìš©ì…ë‹ˆë‹¤.\n"
            knowledge += "ì´ ì •ë³´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì—…ê³„ íŠ¸ë Œë“œ, Pain Point, ì‚¬ë¡€ ë“±ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•˜ì„¸ìš”.\n\n"
            
            # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            all_keywords = []
            for row in rows:
                keywords = row[2].split(',') if row[2] else []
                all_keywords.extend(keywords)
            
            # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
            from collections import Counter
            keyword_freq = Counter(all_keywords)
            top_keywords = [k for k, v in keyword_freq.most_common(10)]
            
            knowledge += f"**ì£¼ìš” í‚¤ì›Œë“œ**: {', '.join(top_keywords)}\n\n"
            
            # ëŒ€í‘œ ê¸€ 5ê°œ ìš”ì•½
            knowledge += f"**ëŒ€í‘œ ì¸ì‚¬ì´íŠ¸:**\n\n"
            for i, row in enumerate(rows[:5], 1):
                title, summary = row[0], row[1]
                knowledge += f"{i}. {title}\n"
                if summary:
                    knowledge += f"   â†’ {summary[:150]}...\n\n"
            
            logger.info(f"âœ… {service_type} ë¸”ë¡œê·¸ {len(rows)}ê°œ ìš”ì•½ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"ë¸”ë¡œê·¸ ìš”ì•½ ì˜¤ë¥˜: {str(e)}")
    
    # 3. RAG í™œìš© ì§€ì¹¨
    knowledge += f"\n\n**ğŸ’¡ ì§€ì‹ í™œìš© ê°€ì´ë“œ:**\n"
    knowledge += "- ìœ„ ì„œë¹„ìŠ¤ ì†Œê°œì„œì™€ ë¸”ë¡œê·¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ê¹Šì´ ì´í•´í•˜ê³  í™œìš©í•˜ì„¸ìš”\n"
    knowledge += "- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ê¸°ëŠ¥, íš¨ê³¼ë¥¼ ì •í™•í•˜ê²Œ ì–¸ê¸‰í•˜ì„¸ìš”\n"
    knowledge += "- ì—…ê³„ íŠ¸ë Œë“œë‚˜ Pain PointëŠ” 'ì—…ê³„ì—ì„œëŠ”...', 'ë§ì€ ê¸°ì—…ë“¤ì´...' í˜•íƒœë¡œ ìì—°ìŠ¤ëŸ½ê²Œ\n"
    knowledge += "- ê²½ìŸë ¥ ìˆëŠ” ì°¨ë³„ì ê³¼ í•µì‹¬ ê°€ì¹˜ë¥¼ ëª…í™•íˆ ì „ë‹¬í•˜ì„¸ìš”\n"
    knowledge += f"- {service_type} ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì „ë¬¸ì„±ê³¼ ì‹ ë¢°ì„±ì„ ë³´ì—¬ì£¼ì„¸ìš”\n"
    
    return knowledge
